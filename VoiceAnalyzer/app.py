import sys
import os
from pathlib import Path
from PySide6.QtWidgets import (
    QApplication, QWidget, QPushButton, QVBoxLayout, QLabel, QFileDialog,
    QTextEdit, QScrollArea, QProgressBar, QMessageBox, QDialog, QComboBox,
    QDialogButtonBox, QFormLayout, QHBoxLayout, QTabWidget, QListWidget, 
    QListWidgetItem, QCheckBox
)
from PySide6.QtCore import Qt, QThread, Signal
from PySide6.QtGui import QColor
import subprocess
import json
import collections
import webbrowser
import tempfile
import time
import requests
from tqdm import tqdm
import zipfile
import shutil

# Whisper Ùˆ ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ†
import whisper

# ØªØµØ­ÛŒØ­ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ§Ø±Ø³ÛŒ
try:
    from persiantools import digits
    from persiantools import characters
    PERSIAN_TOOLS_AVAILABLE = True
except ImportError:
    PERSIAN_TOOLS_AVAILABLE = False
    print("Warning: persiantools not installed. Install with: pip install persiantools")

# Google Speech-to-Text
try:
    from google.cloud import speech
    GOOGLE_SPEECH_AVAILABLE = True
except ImportError:
    GOOGLE_SPEECH_AVAILABLE = False
    print("Warning: google-cloud-speech not installed. Install with: pip install google-cloud-speech")

# Vosk Speech Recognition
try:
    import vosk
    import json
    VOSK_AVAILABLE = True
except ImportError:
    VOSK_AVAILABLE = False
    print("Warning: vosk not installed. Install with: pip install vosk")

# Microsoft Azure Speech
try:
    import azure.cognitiveservices.speech as speechsdk
    AZURE_SPEECH_AVAILABLE = True
except ImportError:
    AZURE_SPEECH_AVAILABLE = False
    print("Warning: azure-cognitiveservices-speech not installed. Install with: pip install azure-cognitiveservices-speech")

# AssemblyAI
try:
    import assemblyai as aai
    ASSEMBLYAI_AVAILABLE = True
except ImportError:
    ASSEMBLYAI_AVAILABLE = False
    print("Warning: assemblyai not installed. Install with: pip install assemblyai")

# Hugging Face Transformers
try:
    from transformers import pipeline, AutoModelForCTC, AutoProcessor
    import torch
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False
    print("Warning: transformers not installed. Install with: pip install transformers torch")

# SpeechRecognition
try:
    import speech_recognition as sr
    SPEECHRECOGNITION_AVAILABLE = True
except ImportError:
    SPEECHRECOGNITION_AVAILABLE = False
    print("Warning: SpeechRecognition not installed. Install with: pip install SpeechRecognition")

# Silero STT
try:
    import torch
    import torchaudio
    SILERO_AVAILABLE = True
except ImportError:
    SILERO_AVAILABLE = False
    print("Warning: torchaudio not installed. Install with: pip install torchaudio")

# Kaldi
try:
    import kaldi_io
    KALDI_AVAILABLE = True
except ImportError:
    KALDI_AVAILABLE = False
    print("Warning: kaldi-io not installed. Install with: pip install kaldi-io")

# Ø¨Ø±Ø±Ø³ÛŒ ffmpeg Ø¯Ø± PATH
try:
    subprocess.run(["ffmpeg", "-version"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
except FileNotFoundError:
    raise RuntimeError("FFmpeg not found in PATH! Install it or add to PATH.")

# Ù…Ø³ÛŒØ± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ú©Ø§Ø³ØªÙˆÙ…
CUSTOM_DICT_FILE = Path("custom_dict.json")
if not CUSTOM_DICT_FILE.exists():
    with open(CUSTOM_DICT_FILE, "w", encoding="utf-8") as f:
        json.dump({}, f, ensure_ascii=False, indent=2)

# Ù…Ø³ÛŒØ± ØªÚ©ÛŒÙ‡ Ú©Ù„Ø§Ù…â€ŒÙ‡Ø§
RELYING_DICT_FILE = Path("relying_dict.json")
if not RELYING_DICT_FILE.exists():
    with open(RELYING_DICT_FILE, "w", encoding="utf-8") as f:
        json.dump({"ØªÚ©ÛŒÙ‡_Ú©Ù„Ø§Ù…": ["Ø®Ø¨", "Ø¢Ø®Ù‡", "ÛŒØ¹Ù†ÛŒ", "Ø­Ø§Ù„Ø§"]}, f, ensure_ascii=False, indent=2)

# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
CONFIG_FILE = Path("voice_analyzer_config.json")
if not CONFIG_FILE.exists():
    with open(CONFIG_FILE, "w", encoding="utf-8") as f:
        json.dump({
            "selected_model": "vosk_persian",
            "last_audio_path": "",
            "window_geometry": None,
            "preferred_language": "fa"
        }, f, ensure_ascii=False, indent=2)

# Import Ù¾Ù†Ø¬Ø±Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ
from custom_dict_manager import DictManager

class ConfigManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    
    @staticmethod
    def load_config():
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„"""
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª: {e}")
            return {
                "selected_model": "vosk_persian",
                "last_audio_path": "",
                "window_geometry": None,
                "preferred_language": "fa"
            }
    
    @staticmethod
    def save_config(config):
        """Ø°Ø®ÛŒØ±Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¯Ø± ÙØ§ÛŒÙ„"""
        try:
            with open(CONFIG_FILE, "w", encoding="utf-8") as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª: {e}")
            return False
    
    @staticmethod
    def update_model(model_name):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡"""
        config = ConfigManager.load_config()
        config["selected_model"] = model_name
        return ConfigManager.save_config(config)
    
    @staticmethod
    def update_audio_path(audio_path):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø³ÛŒØ± Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ"""
        config = ConfigManager.load_config()
        config["last_audio_path"] = audio_path
        return ConfigManager.save_config(config)
    
    @staticmethod
    def update_window_geometry(geometry):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØª Ùˆ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡"""
        config = ConfigManager.load_config()
        config["window_geometry"] = geometry
        return ConfigManager.save_config(config)

class ModelDownloader:
    """Ú©Ù„Ø§Ø³ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
    
    # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯
    DOWNLOADABLE_MODELS = {
        # Vosk Models (ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)
        "vosk_persian": {
            "url": "https://alphacephei.com/vosk/models/vosk-model-fa-0.5.zip",
            "name": "vosk-model-fa-0.5", 
            "size": "1.13 GB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "âœ… Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ",
            "type": "Vosk"
        },
        "vosk_small": {
            "url": "https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip",
            "name": "vosk-model-small-en-us-0.15",
            "size": "40 MB",
            "language": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "warning": "âš ï¸ ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "type": "Vosk"
        },
        "vosk_large": {
            "url": "https://alphacephei.com/vosk/models/vosk-model-en-us-0.22.zip", 
            "name": "vosk-model-en-us-0.22",
            "size": "1.8 GB",
            "language": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "warning": "âš ï¸ ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "type": "Vosk"
        },
        
        # Whisper Models
        "whisper_tiny": {
            "url": "whisper://tiny",
            "name": "whisper-tiny",
            "size": "75 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âš ï¸ Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ",
            "type": "Whisper"
        },
        "whisper_base": {
            "url": "whisper://base",
            "name": "whisper-base",
            "size": "142 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âš ï¸ Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ",
            "type": "Whisper"
        },
        "whisper_small": {
            "url": "whisper://small",
            "name": "whisper-small",
            "size": "466 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… ØªØ¹Ø§Ø¯Ù„ Ø®ÙˆØ¨",
            "type": "Whisper"
        },
        "whisper_medium": {
            "url": "whisper://medium",
            "name": "whisper-medium",
            "size": "1.5 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§",
            "type": "Whisper"
        },
        "whisper_large": {
            "url": "whisper://large",
            "name": "whisper-large",
            "size": "2.9 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¯Ù‚Øª",
            "type": "Whisper"
        },
        "whisper_large_v2": {
            "url": "whisper://large-v2",
            "name": "whisper-large-v2",
            "size": "2.9 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡",
            "type": "Whisper"
        },
        "whisper_large_v3": {
            "url": "whisper://large-v3",
            "name": "whisper-large-v3",
            "size": "2.9 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡",
            "type": "Whisper"
        },
        
        # Hugging Face Transformers
        "hf_wav2vec2_persian": {
            "url": "huggingface://m3hrdadfi/wav2vec2-large-xlsr-53-persian",
            "name": "wav2vec2-large-xlsr-53-persian",
            "size": "1.2 GB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "âœ… Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ - Hugging Face",
            "type": "HuggingFace"
        },
        "hf_whisper_persian": {
            "url": "huggingface://m3hrdadfi/whisper-persian",
            "name": "whisper-persian",
            "size": "1.5 GB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "âœ… Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ - Hugging Face",
            "type": "HuggingFace"
        },
        "hf_wav2vec2_persian_alt": {
            "url": "huggingface://facebook/wav2vec2-large-xlsr-53",
            "name": "wav2vec2-large-xlsr-53",
            "size": "1.2 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âš ï¸ Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡ - Ù†ÛŒØ§Ø² Ø¨Ù‡ fine-tuning Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ",
            "type": "HuggingFace"
        },
        "hf_whisper_tiny": {
            "url": "huggingface://openai/whisper-tiny",
            "name": "whisper-tiny-hf",
            "size": "75 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âš ï¸ Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ",
            "type": "HuggingFace"
        },
        "hf_whisper_base": {
            "url": "huggingface://openai/whisper-base",
            "name": "whisper-base-hf",
            "size": "142 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âš ï¸ Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ",
            "type": "HuggingFace"
        },
        "hf_whisper_small": {
            "url": "huggingface://openai/whisper-small",
            "name": "whisper-small-hf",
            "size": "466 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… ØªØ¹Ø§Ø¯Ù„ Ø®ÙˆØ¨",
            "type": "HuggingFace"
        },
        "hf_whisper_medium": {
            "url": "huggingface://openai/whisper-medium",
            "name": "whisper-medium-hf",
            "size": "1.5 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§",
            "type": "HuggingFace"
        },
        "hf_whisper_large": {
            "url": "huggingface://openai/whisper-large-v2",
            "name": "whisper-large-v2-hf",
            "size": "2.9 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¯Ù‚Øª",
            "type": "HuggingFace"
        },
        
        # SpeechRecognition
        "speechrecognition_google": {
            "url": "speechrecognition://google",
            "name": "Google Speech Recognition",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† 60Ø¯Ù‚ÛŒÙ‚Ù‡/Ù…Ø§Ù‡",
            "type": "SpeechRecognition"
        },
        "speechrecognition_sphinx": {
            "url": "speechrecognition://sphinx",
            "name": "CMU Sphinx",
            "size": "100 MB",
            "language": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "warning": "âš ï¸ ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "type": "SpeechRecognition"
        },
        "speechrecognition_wit": {
            "url": "speechrecognition://wit",
            "name": "Wit.ai",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ",
            "type": "SpeechRecognition"
        },
        "speechrecognition_azure": {
            "url": "speechrecognition://azure",
            "name": "Azure Speech",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† 5Ø³Ø§Ø¹Øª/Ù…Ø§Ù‡",
            "type": "SpeechRecognition"
        },
        "speechrecognition_bing": {
            "url": "speechrecognition://bing",
            "name": "Bing Speech",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ",
            "type": "SpeechRecognition"
        },
        "speechrecognition_houndify": {
            "url": "speechrecognition://houndify",
            "name": "Houndify",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ",
            "type": "SpeechRecognition"
        },
        "speechrecognition_ibm": {
            "url": "speechrecognition://ibm",
            "name": "IBM Speech to Text",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ",
            "type": "SpeechRecognition"
        },
        
        # Silero STT
        "silero_stt_en": {
            "url": "silero://stt_en",
            "name": "Silero STT English",
            "size": "50 MB",
            "language": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "warning": "âš ï¸ ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "type": "Silero"
        },
        "silero_stt_multilingual": {
            "url": "silero://stt_multilingual",
            "name": "Silero STT Multilingual",
            "size": "200 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ÙØ§Ø±Ø³ÛŒ",
            "type": "Silero"
        },
        
        # Kaldi
        "kaldi_persian": {
            "url": "kaldi://persian",
            "name": "Kaldi Persian Model",
            "size": "500 MB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "ğŸš§ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡ - Ø§Ø² Vosk Persian Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯",
            "type": "Kaldi"
        },
        "kaldi_english": {
            "url": "kaldi://english",
            "name": "Kaldi English Model",
            "size": "300 MB",
            "language": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "warning": "ğŸš§ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡ - Ø§Ø² Vosk Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯",
            "type": "Kaldi"
        },
        
        # Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¨ÙˆÙ…ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
        "iranian_arvan": {
            "url": "arvan://speech",
            "name": "Arvan Cloud Speech",
            "size": "0 MB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "ğŸ‡®ğŸ‡· Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ - Ø¢Ù†Ù„Ø§ÛŒÙ†",
            "type": "Iranian"
        },
        "iranian_fanap": {
            "url": "fanap://speech",
            "name": "Fanap Speech API",
            "size": "0 MB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "ğŸ‡®ğŸ‡· Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ - Ø¢Ù†Ù„Ø§ÛŒÙ†",
            "type": "Iranian"
        },
        "iranian_parsijoo": {
            "url": "parsijoo://speech",
            "name": "Parsijoo Speech",
            "size": "0 MB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "ğŸ‡®ğŸ‡· Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ - Ø¢Ù†Ù„Ø§ÛŒÙ†",
            "type": "Iranian"
        }
    }
    
    # Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ
    VOSK_MODELS = {
        key: value for key, value in DOWNLOADABLE_MODELS.items() 
        if value["type"] == "Vosk"
    }
    
    @staticmethod
    def download_model(model_id, progress_callback=None, progress_bar_callback=None):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„"""
        if model_id not in ModelDownloader.DOWNLOADABLE_MODELS:
            return False, f"Ù…Ø¯Ù„ {model_id} Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯"
        
        model_info = ModelDownloader.DOWNLOADABLE_MODELS[model_id]
        
        # Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper
        if model_info["type"] == "Whisper":
            return ModelDownloader._download_whisper_model(model_id, model_info, progress_callback, progress_bar_callback)
        
        # Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk
        elif model_info["type"] == "Vosk":
            return ModelDownloader._download_vosk_model(model_id, model_info, progress_callback, progress_bar_callback)
        
        return False, f"Ù†ÙˆØ¹ Ù…Ø¯Ù„ {model_info['type']} Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯"
    
    @staticmethod
    def _download_whisper_model(model_id, model_info, progress_callback=None, progress_bar_callback=None):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Whisper"""
        try:
            if progress_callback:
                progress_callback(f"Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_info['name']} ({model_info['size']})...")
            
            # Whisper Ø®ÙˆØ¯Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
            model_name = model_id.replace("whisper_", "")
            # ØªØ¨Ø¯ÛŒÙ„ Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Whisper
            if model_name == "large_v2":
                model_name = "large-v2"
            elif model_name == "large_v3":
                model_name = "large-v3"
            
            model = whisper.load_model(model_name)
            
            if progress_bar_callback:
                progress_bar_callback(100)
            
            return True, f"Ù…Ø¯Ù„ {model_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯"
            
        except Exception as e:
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ Whisper: {str(e)}"
    
    @staticmethod
    def _download_vosk_model(model_id, model_info, progress_callback=None, progress_bar_callback=None):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Vosk"""
        models_dir = Path.home() / ".vosk" / "models"
        models_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = models_dir / model_info["name"]
        
        # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡
        if model_path.exists():
            return True, str(model_path)
        
        try:
            # Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„
            if progress_callback:
                progress_callback(f"Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_info['name']} ({model_info['size']})...")
            
            response = requests.get(model_info["url"], stream=True)
            response.raise_for_status()
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0
            
            # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„
            zip_path = models_dir / f"{model_info['name']}.zip"
            
            with open(zip_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        
                        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ progress bar
                        if progress_bar_callback and total_size > 0:
                            progress_percent = int((downloaded_size / total_size) * 100)
                            progress_bar_callback(progress_percent)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§ÛŒÙ„
            if progress_callback:
                progress_callback("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¯Ù„...")
            
            if progress_bar_callback:
                progress_bar_callback(100)  # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(models_dir)
            
            # Ø­Ø°Ù ÙØ§ÛŒÙ„ zip
            zip_path.unlink()
            
            return True, str(model_path)
            
        except Exception as e:
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯: {str(e)}"
    
    @staticmethod
    def is_model_downloaded(model_id):
        """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„"""
        if model_id not in ModelDownloader.DOWNLOADABLE_MODELS:
            return False
        
        model_info = ModelDownloader.DOWNLOADABLE_MODELS[model_id]
        
        # Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper
        if model_info["type"] == "Whisper":
            try:
                model_name = model_id.replace("whisper_", "")
                # ØªØ¨Ø¯ÛŒÙ„ Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Whisper
                if model_name == "large_v2":
                    model_name = "large-v2"
                elif model_name == "large_v3":
                    model_name = "large-v3"
                
                # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„ Ø¯Ø± cache Whisper
                import whisper
                model_path = whisper._MODELS.get(model_name)
                return model_path is not None
            except:
                return False
        
        # Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk
        elif model_info["type"] == "Vosk":
            models_dir = Path.home() / ".vosk" / "models"
            model_path = models_dir / model_info["name"]
            return model_path.exists()
        
        return False

class ModelSelectionDialog(QDialog):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Speech-to-Text")
        self.setModal(True)
        self.resize(600, 500)
        
        layout = QVBoxLayout(self)
        
        # ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„
        filter_layout = QHBoxLayout()
        
        # Ú†Ú© Ø¨Ø§Ú©Ø³ Ø²Ø¨Ø§Ù†
        self.checkbox_persian = QCheckBox("ÙØ§Ø±Ø³ÛŒ")
        self.checkbox_persian.setChecked(True)
        self.checkbox_persian.setStyleSheet("QCheckBox { font-weight: bold; color: #2e7d32; }")
        self.checkbox_persian.stateChanged.connect(self.filter_models)
        
        self.checkbox_english = QCheckBox("Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ")
        self.checkbox_english.setChecked(True)
        self.checkbox_english.setStyleSheet("QCheckBox { font-weight: bold; color: #1976d2; }")
        self.checkbox_english.stateChanged.connect(self.filter_models)
        
        # Ú†Ú© Ø¨Ø§Ú©Ø³ Ù†ÙˆØ¹ Ø§ØªØµØ§Ù„
        self.checkbox_online = QCheckBox("Ø¢Ù†Ù„Ø§ÛŒÙ†")
        self.checkbox_online.setChecked(True)
        self.checkbox_online.setStyleSheet("QCheckBox { font-weight: bold; color: #ff6b35; }")
        self.checkbox_online.stateChanged.connect(self.filter_models)
        
        self.checkbox_offline = QCheckBox("Ø¢ÙÙ„Ø§ÛŒÙ†")
        self.checkbox_offline.setChecked(True)
        self.checkbox_offline.setStyleSheet("QCheckBox { font-weight: bold; color: #9c27b0; }")
        self.checkbox_offline.stateChanged.connect(self.filter_models)
        
        filter_layout.addWidget(QLabel("Ø²Ø¨Ø§Ù†:"))
        filter_layout.addWidget(self.checkbox_persian)
        filter_layout.addWidget(self.checkbox_english)
        filter_layout.addStretch()
        filter_layout.addWidget(QLabel("Ù†ÙˆØ¹:"))
        filter_layout.addWidget(self.checkbox_online)
        filter_layout.addWidget(self.checkbox_offline)
        
        layout.addLayout(filter_layout)
        
        # Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Speech-to-Text (ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)
        self.model_list = QListWidget()
        self.model_list.setMinimumHeight(300)
        self.model_list.setStyleSheet("""
            QListWidget {
                background-color: #f5f5f5;
                border: 1px solid #ddd;
                border-radius: 5px;
                padding: 5px;
            }
            QListWidget::item {
                padding: 8px;
                border-bottom: 1px solid #eee;
            }
            QListWidget::item:selected {
                background-color: #e3f2fd;
                color: #1976d2;
            }
        """)
        layout.addWidget(self.model_list)
        
        # Ø°Ø®ÛŒØ±Ù‡ ØªÙ…Ø§Ù… Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù†
        self.all_models = [
            # Vosk Models (Ø¢ÙÙ„Ø§ÛŒÙ†)
            ("vosk_persian", "âœ… Vosk Persian - Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ (1.13 GB)", "persian", "offline"),
            ("vosk_small", "âš ï¸ Vosk Small - ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (40 MB)", "english", "offline"),
            ("vosk_large", "âš ï¸ Vosk Large - ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (1.8 GB)", "english", "offline"),
            
            # Whisper Models (Ø¢ÙÙ„Ø§ÛŒÙ† - Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡)
            ("whisper_tiny", "âš ï¸ Whisper Tiny - Ø®ÛŒÙ„ÛŒ Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ (75 MB)", "both", "offline"),
            ("whisper_base", "âš ï¸ Whisper Base - Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ (142 MB)", "both", "offline"),
            ("whisper_small", "âœ… Whisper Small - ØªØ¹Ø§Ø¯Ù„ Ø®ÙˆØ¨ (466 MB)", "both", "offline"),
            ("whisper_medium", "âœ… Whisper Medium - Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ (1.5 GB)", "both", "offline"),
            ("whisper_large", "âœ… Whisper Large - Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¯Ù‚Øª (2.9 GB)", "both", "offline"),
            ("whisper_large_v2", "âœ… Whisper Large V2 - Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡ (2.9 GB)", "both", "offline"),
            ("whisper_large_v3", "âœ… Whisper Large V3 - Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡ (2.9 GB)", "both", "offline"),
            
            # Hugging Face Transformers (Ø¢ÙÙ„Ø§ÛŒÙ†)
            ("hf_wav2vec2_persian", "âœ… Wav2Vec2 Persian - Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ (1.2 GB)", "persian", "offline"),
            ("hf_whisper_persian", "âœ… Whisper Persian - Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ (1.5 GB)", "persian", "offline"),
            ("hf_wav2vec2_persian_alt", "âš ï¸ Wav2Vec2 Multilingual - Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡ (1.2 GB)", "both", "offline"),
            ("hf_whisper_tiny", "âš ï¸ Whisper Tiny HF - Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ (75 MB)", "both", "offline"),
            ("hf_whisper_base", "âš ï¸ Whisper Base HF - Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ (142 MB)", "both", "offline"),
            ("hf_whisper_small", "âœ… Whisper Small HF - ØªØ¹Ø§Ø¯Ù„ Ø®ÙˆØ¨ (466 MB)", "both", "offline"),
            ("hf_whisper_medium", "âœ… Whisper Medium HF - Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ (1.5 GB)", "both", "offline"),
            ("hf_whisper_large", "âœ… Whisper Large HF - Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¯Ù‚Øª (2.9 GB)", "both", "offline"),
            
            # SpeechRecognition (Ø¢Ù†Ù„Ø§ÛŒÙ†)
            ("speechrecognition_google", "ğŸŒ Google Speech - Ø±Ø§ÛŒÚ¯Ø§Ù† 60Ø¯Ù‚ÛŒÙ‚Ù‡/Ù…Ø§Ù‡ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("speechrecognition_sphinx", "âš ï¸ CMU Sphinx - ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (100 MB)", "english", "offline"),
            ("speechrecognition_wit", "ğŸŒ Wit.ai - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("speechrecognition_azure", "ğŸŒ Azure Speech - Ø±Ø§ÛŒÚ¯Ø§Ù† 5Ø³Ø§Ø¹Øª/Ù…Ø§Ù‡ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("speechrecognition_bing", "ğŸŒ Bing Speech - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("speechrecognition_houndify", "ğŸŒ Houndify - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("speechrecognition_ibm", "ğŸŒ IBM Speech - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            
            # Silero STT (Ø¢ÙÙ„Ø§ÛŒÙ†)
            ("silero_stt_en", "âš ï¸ Silero STT English - ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (50 MB)", "english", "offline"),
            ("silero_stt_multilingual", "âœ… Silero STT Multilingual - Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ÙØ§Ø±Ø³ÛŒ (200 MB)", "both", "offline"),
            
            # Kaldi (Ø¢ÙÙ„Ø§ÛŒÙ†)
            ("kaldi_persian", "ğŸš§ Kaldi Persian - Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡ (500 MB)", "persian", "offline"),
            ("kaldi_english", "ğŸš§ Kaldi English - Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡ (300 MB)", "english", "offline"),
            
            # Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¨ÙˆÙ…ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)
            ("iranian_arvan", "ğŸ‡®ğŸ‡· Arvan Cloud Speech - Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "persian", "online"),
            ("iranian_fanap", "ğŸ‡®ğŸ‡· Fanap Speech API - Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "persian", "online"),
            ("iranian_parsijoo", "ğŸ‡®ğŸ‡· Parsijoo Speech - Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "persian", "online"),
            
            # Google Models (Ø¢Ù†Ù„Ø§ÛŒÙ†)
            ("google_standard", "ğŸŒ Google Standard - Ø±Ø§ÛŒÚ¯Ø§Ù† 60Ø¯Ù‚ÛŒÙ‚Ù‡/Ù…Ø§Ù‡ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("google_enhanced", "ğŸ’³ Google Enhanced - Ù¾ÙˆÙ„ÛŒØŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ± (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("google_phone_call", "ğŸ’³ Google Phone Call - Ù¾ÙˆÙ„ÛŒØŒ Ù…Ø®ØµÙˆØµ ØªÙ…Ø§Ø³â€ŒÙ‡Ø§ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("google_medical", "ğŸ’³ Google Medical - Ù¾ÙˆÙ„ÛŒØŒ Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ù¾Ø²Ø´Ú©ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("google_video", "ğŸ’³ Google Video - Ù¾ÙˆÙ„ÛŒØŒ Ù…Ø®ØµÙˆØµ ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            
            # Microsoft Azure Speech (Ø¢Ù†Ù„Ø§ÛŒÙ†)
            ("azure_standard", "ğŸŒ Azure Standard - Ø±Ø§ÛŒÚ¯Ø§Ù† 5Ø³Ø§Ø¹Øª/Ù…Ø§Ù‡ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("azure_enhanced", "ğŸ’³ Azure Enhanced - Ù¾ÙˆÙ„ÛŒØŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ± (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            
            # AssemblyAI (Ø¢Ù†Ù„Ø§ÛŒÙ†)
            ("assemblyai_standard", "ğŸŒ AssemblyAI - Ø±Ø§ÛŒÚ¯Ø§Ù† 3Ø³Ø§Ø¹Øª/Ù…Ø§Ù‡ (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online"),
            ("assemblyai_enhanced", "ğŸ’³ AssemblyAI Enhanced - Ù¾ÙˆÙ„ÛŒØŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ± (Ø¢Ù†Ù„Ø§ÛŒÙ†)", "both", "online")
        ]
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¯Ø± Ù„ÛŒØ³Øª
        self.populate_model_list()
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
        config = ConfigManager.load_config()
        last_model = config.get("selected_model", "vosk_persian")
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Ø¢Ø®Ø±
        for i in range(self.model_list.count()):
            item = self.model_list.item(i)
            if item.data(Qt.UserRole) == last_model:
                self.model_list.setCurrentItem(item)
                break
        
        # Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
        button_layout = QHBoxLayout()
        self.ok_button = QPushButton("ØªØ£ÛŒÛŒØ¯")
        self.ok_button.setStyleSheet("background-color: #4CAF50; color: white; padding: 8px 16px;")
        self.ok_button.clicked.connect(self.accept)
        
        self.cancel_button = QPushButton("Ù„ØºÙˆ")
        self.cancel_button.setStyleSheet("background-color: #f44336; color: white; padding: 8px 16px;")
        self.cancel_button.clicked.connect(self.reject)
        
        button_layout.addWidget(self.ok_button)
        button_layout.addWidget(self.cancel_button)
        layout.addLayout(button_layout)
    
    def populate_model_list(self):
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¯Ø± Ù„ÛŒØ³Øª Ø¨Ø± Ø§Ø³Ø§Ø³ ÙÛŒÙ„ØªØ±Ù‡Ø§"""
        self.model_list.clear()
        
        for model_id, description, language, connection_type in self.all_models:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙÛŒÙ„ØªØ± Ø²Ø¨Ø§Ù†
            language_match = False
            if language == "both":
                language_match = True
            elif language == "persian" and self.checkbox_persian.isChecked():
                language_match = True
            elif language == "english" and self.checkbox_english.isChecked():
                language_match = True
            
            # Ø¨Ø±Ø±Ø³ÛŒ ÙÛŒÙ„ØªØ± Ù†ÙˆØ¹ Ø§ØªØµØ§Ù„
            connection_match = False
            if connection_type == "online" and self.checkbox_online.isChecked():
                connection_match = True
            elif connection_type == "offline" and self.checkbox_offline.isChecked():
                connection_match = True
            
            # Ø§Ú¯Ø± Ù‡Ø± Ø¯Ùˆ ÙÛŒÙ„ØªØ± Ù…Ø·Ø§Ø¨Ù‚Øª Ø¯Ø§Ø´ØªØŒ Ù…Ø¯Ù„ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†
            if language_match and connection_match:
                item = QListWidgetItem(f"{model_id} - {description}")
                item.setData(Qt.UserRole, model_id)
                
                # Ø±Ù†Ú¯â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
                if connection_type == "online":
                    item.setBackground(QColor("#fff3e0"))  # Ù†Ø§Ø±Ù†Ø¬ÛŒ Ø±ÙˆØ´Ù†
                else:
                    item.setBackground(QColor("#f3e5f5"))  # Ø¨Ù†ÙØ´ Ø±ÙˆØ´Ù†
                
                self.model_list.addItem(item)
    
    def filter_models(self):
        """ÙÛŒÙ„ØªØ± Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ú†Ú© Ø¨Ø§Ú©Ø³â€ŒÙ‡Ø§"""
        self.populate_model_list()
    
    def get_selected_model(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡"""
        current_item = self.model_list.currentItem()
        if current_item:
            return current_item.data(Qt.UserRole)
        return None

def improve_persian_text(text):
    """Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ ØªØµØ­ÛŒØ­ Ø®ÙˆØ¯Ú©Ø§Ø±"""
    if not PERSIAN_TOOLS_AVAILABLE:
        return text
    
    # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ú©Ù„Ù…Ø§Øª
    words = text.split()
    cleaned_words = []
    prev_word = None
    repeat_count = 0
    
    for word in words:
        if word == prev_word:
            repeat_count += 1
            if repeat_count < 3:  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 ØªÚ©Ø±Ø§Ø± Ù…Ø¬Ø§Ø²
                cleaned_words.append(word)
        else:
            repeat_count = 0
            cleaned_words.append(word)
        prev_word = word
    
    text = " ".join(cleaned_words)
    
    # ØªØµØ­ÛŒØ­ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
    text = characters.ar_to_fa(text)  # ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
    
    # ØªØµØ­ÛŒØ­ Ø§Ø¹Ø¯Ø§Ø¯
    text = digits.en_to_fa(text)  # ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
    
    return text

class TranscribeThread(QThread):
    progress = Signal(int)
    finished = Signal(str)
    download_progress = Signal(int)
    download_status = Signal(str)

    def __init__(self, audio_path, model_name="vosk_persian"):
        super().__init__()
        self.audio_path = audio_path
        self.model_name = model_name
        self.model = None
        self._pause = False
        self._stop = False

    def run(self):
        try:
            self.progress.emit(5)
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
            if self.model_name.startswith("whisper_"):
                whisper_model = self.model_name.replace("whisper_", "")
                # ØªØ¨Ø¯ÛŒÙ„ Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Whisper
                if whisper_model == "large_v2":
                    whisper_model = "large-v2"
                elif whisper_model == "large_v3":
                    whisper_model = "large-v3"
                self.model = whisper.load_model(whisper_model)
            elif self.model_name.startswith("google_"):
                if not GOOGLE_SPEECH_AVAILABLE:
                    self.finished.emit("Error: Google Speech-to-Text not installed. Install with: pip install google-cloud-speech")
                    return
                self.model = "google"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Google
            elif self.model_name.startswith("vosk_"):
                if not VOSK_AVAILABLE:
                    self.finished.emit("Error: Vosk not installed. Install with: pip install vosk")
                    return
                self.model = "vosk"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Vosk
            elif self.model_name.startswith("azure_"):
                if not AZURE_SPEECH_AVAILABLE:
                    self.finished.emit("Error: Azure Speech not installed. Install with: pip install azure-cognitiveservices-speech")
                    return
                self.model = "azure"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Azure
            elif self.model_name.startswith("assemblyai_"):
                if not ASSEMBLYAI_AVAILABLE:
                    self.finished.emit("Error: AssemblyAI not installed. Install with: pip install assemblyai")
                    return
                self.model = "assemblyai"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AssemblyAI
            elif self.model_name.startswith("hf_"):
                if not HUGGINGFACE_AVAILABLE:
                    self.finished.emit("Error: Hugging Face Transformers not installed. Install with: pip install transformers torch")
                    return
                self.model = "huggingface"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Hugging Face
            elif self.model_name.startswith("speechrecognition_"):
                if not SPEECHRECOGNITION_AVAILABLE:
                    self.finished.emit("Error: SpeechRecognition not installed. Install with: pip install SpeechRecognition")
                    return
                self.model = "speechrecognition"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² SpeechRecognition
            elif self.model_name.startswith("silero_"):
                if not SILERO_AVAILABLE:
                    self.finished.emit("Error: Silero STT not installed. Install with: pip install torchaudio")
                    return
                self.model = "silero"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Silero
            elif self.model_name.startswith("kaldi_"):
                if not KALDI_AVAILABLE:
                    self.finished.emit("Error: Kaldi not installed. Install with: pip install kaldi-io")
                    return
                self.model = "kaldi"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Kaldi
            elif self.model_name.startswith("iranian_"):
                self.model = "iranian"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
            else:
                self.finished.emit(f"Error: Unknown model {self.model_name}")
                return
                
            self.progress.emit(10)
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ WAV 16kHz Mono Ø¯Ø± ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                temp_wav = tmp.name

            subprocess.run([
                "ffmpeg", "-y", "-i", str(self.audio_path),
                "-ar", "16000", "-ac", "1", 
                "-af", "highpass=f=80,lowpass=f=8000",  # ÙÛŒÙ„ØªØ± ØµØ¯Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª
                temp_wav
            ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.progress.emit(35)

            # ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ†
            if self.model == "google":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Google Speech-to-Text
                text = self.transcribe_with_google(temp_wav)
            elif self.model == "vosk":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Vosk
                text = self.transcribe_with_vosk(temp_wav)
            elif self.model == "azure":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Azure Speech
                text = self.transcribe_with_azure(temp_wav)
            elif self.model == "assemblyai":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AssemblyAI
                text = self.transcribe_with_assemblyai(temp_wav)
            elif self.model == "huggingface":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Hugging Face
                text = self.transcribe_with_huggingface(temp_wav)
            elif self.model == "speechrecognition":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² SpeechRecognition
                text = self.transcribe_with_speechrecognition(temp_wav)
            elif self.model == "silero":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Silero STT
                text = self.transcribe_with_silero(temp_wav)
            elif self.model == "kaldi":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Kaldi
                text = self.transcribe_with_kaldi(temp_wav)
            elif self.model == "iranian":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
                text = self.transcribe_with_iranian(temp_wav)
            else:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Whisper
                result = self.model.transcribe(
                    temp_wav,
                    language="fa",  # Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ
                    initial_prompt="Ø§ÛŒÙ† ÛŒÚ© ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† Ø±Ø§ Ø¨Ø§ Ø§Ù…Ù„Ø§ÛŒ ØµØ­ÛŒØ­ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.",
                    temperature=0.0,  # Ú©Ø§Ù‡Ø´ Ø®Ù„Ø§Ù‚ÛŒØª Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±
                    beam_size=5,  # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª
                    best_of=3,  # ØªØ³Øª Ú†Ù†Ø¯ÛŒÙ† Ø¨Ø§Ø± Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡
                    patience=1.0,  # ØµØ¨Ø± Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
                    length_penalty=1.0,  # ØªÙ†Ø¸ÛŒÙ… Ø·ÙˆÙ„ Ù…ØªÙ†
                    suppress_tokens=[-1],  # Ø­Ø°Ù ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
                    word_timestamps=True,  # ØªØ´Ø®ÛŒØµ Ø¨Ù‡ØªØ± Ú©Ù„Ù…Ø§Øª
                    no_speech_threshold=0.6,  # Ø¢Ø³ØªØ§Ù†Ù‡ ØªØ´Ø®ÛŒØµ Ø³Ú©ÙˆØª
                    logprob_threshold=-1.0,  # Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ú©Ù„Ù…Ø§Øª
                    compression_ratio_threshold=2.4  # Ø¢Ø³ØªØ§Ù†Ù‡ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
                )
                text = result["text"]
            
            # Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
            text = improve_persian_text(text)
            
            self.progress.emit(75)

            # Ù„ØºØ§Øª Ú©Ø§Ø³ØªÙˆÙ…
            with open(CUSTOM_DICT_FILE, encoding="utf-8") as f:
                custom_dict = json.load(f)
            for wrong, correct in custom_dict.items():
                text = text.replace(wrong, correct)

            # Ø¢Ù…Ø§Ø±
            words = text.split()
            word_count = len(words)

            # ØªÚ©ÛŒÙ‡ Ú©Ù„Ø§Ù…â€ŒÙ‡Ø§
            with open(RELYING_DICT_FILE, encoding="utf-8") as f:
                relying_data = json.load(f)
            relying_words = relying_data.get("ØªÚ©ÛŒÙ‡_Ú©Ù„Ø§Ù…", [])
            detected_relying = [w for w in words if w in relying_words]

            counter = collections.Counter(words)
            most_common = counter.most_common(10)

            stats = "\n\n--- Statistics ---\n"
            stats += f"Word Count: {word_count}\n"
            stats += f"Detected Pauses/Tic Words: {', '.join(detected_relying)}\n"
            stats += "Top 10 Frequent Words:\n"
            for w, c in most_common:
                stats += f"{w}: {c}\n"

            text += stats
            self.progress.emit(100)
            self.finished.emit(text)

            # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª
            os.remove(temp_wav)

        except Exception as e:
            self.finished.emit(f"Error: {str(e)}")
    
    def transcribe_with_google(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Google Speech-to-Text"""
        try:
            client = speech.SpeechClient()
            
            # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            with open(audio_file, "rb") as audio_file_content:
                content = audio_file_content.read()
            
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªØ´Ø®ÛŒØµ
            audio = speech.RecognitionAudio(content=content)
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Google
            if self.model_name == "google_enhanced":
                model = "phone_call"  # Ù…Ø¯Ù„ Enhanced
            elif self.model_name == "google_phone_call":
                model = "phone_call"
            elif self.model_name == "google_medical":
                model = "medical_dictation"
            elif self.model_name == "google_video":
                model = "video"
            else:
                model = "default"  # Ù…Ø¯Ù„ Standard
            
            config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=16000,
                language_code="fa-IR",  # ÙØ§Ø±Ø³ÛŒ Ø§ÛŒØ±Ø§Ù†
                model=model,
                enable_automatic_punctuation=True,
                enable_word_time_offsets=True
            )
            
            # ØªØ´Ø®ÛŒØµ
            response = client.recognize(config=config, audio=audio)
            
            # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬
            text = ""
            for result in response.results:
                text += result.alternatives[0].transcript + " "
            
            return text.strip()
            
        except Exception as e:
            error_msg = str(e)
            if "credentials" in error_msg.lower():
                return f"""Google Speech Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… credentials

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Google Speech-to-Text:

1. Ø¨Ù‡ https://console.cloud.google.com Ø¨Ø±ÙˆÛŒØ¯
2. Ù¾Ø±ÙˆÚ˜Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ø³Ø§Ø²ÛŒØ¯ ÛŒØ§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯
3. API Speech-to-Text Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
4. Service Account Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ JSON key Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯
5. Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set GOOGLE_APPLICATION_CREDENTIALS=path/to/your/key.json

ÛŒØ§ Ø§Ø² Whisper Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø¢ÙÙ„Ø§ÛŒÙ† Ùˆ Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
            else:
                return f"Google Speech Error: {error_msg}"
    
    def transcribe_with_vosk(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Vosk"""
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Vosk
            if not ModelDownloader.is_model_downloaded(self.model_name):
                # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø¨Ø§ progress bar
                def progress_callback(message):
                    self.download_status.emit(message)
                
                def progress_bar_callback(percent):
                    self.download_progress.emit(percent)
                
                success, result = ModelDownloader.download_model(
                    self.model_name, 
                    progress_callback=progress_callback,
                    progress_bar_callback=progress_bar_callback
                )
                if not success:
                    return f"Vosk Error: {result}"
            
            # Ø¯Ø±ÛŒØ§ÙØª Ù…Ø³ÛŒØ± Ù…Ø¯Ù„
            model_path = self.get_vosk_model_path()
            if not model_path:
                return "Vosk Error: Ù…Ø¯Ù„ Vosk ÛŒØ§ÙØª Ù†Ø´Ø¯."
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
            model = vosk.Model(model_path)
            
            # ØªÙ†Ø¸ÛŒÙ… Ø²Ø¨Ø§Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
            if self.model_name == "vosk_persian":
                rec = vosk.KaldiRecognizer(model, 16000)
            elif self.model_name in ["vosk_arabic", "vosk_spanish", "vosk_french", "vosk_german", 
                                   "vosk_italian", "vosk_portuguese", "vosk_russian", 
                                   "vosk_chinese", "vosk_japanese", "vosk_korean"]:
                rec = vosk.KaldiRecognizer(model, 16000)
            else:
                rec = vosk.KaldiRecognizer(model, 16000)
            
            # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            with open(audio_file, "rb") as f:
                data = f.read()
            
            # ØªØ´Ø®ÛŒØµ
            if rec.AcceptWaveform(data):
                result = json.loads(rec.Result())
                text = result.get("text", "")
            else:
                result = json.loads(rec.FinalResult())
                text = result.get("text", "")
            
            return text.strip()
            
        except Exception as e:
            return f"Vosk Error: {str(e)}"
    
    def get_vosk_model_path(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ Vosk"""
        if self.model_name not in ModelDownloader.VOSK_MODELS:
            return None
        
        model_info = ModelDownloader.VOSK_MODELS[self.model_name]
        models_dir = Path.home() / ".vosk" / "models"
        model_path = models_dir / model_info["name"]
        
        if model_path.exists():
            return str(model_path)
        
        return None
    
    def transcribe_with_azure(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Azure Speech"""
        try:
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Azure (Ù†ÛŒØ§Ø² Ø¨Ù‡ API Key)
            speech_key = os.getenv("AZURE_SPEECH_KEY")
            service_region = os.getenv("AZURE_SPEECH_REGION", "eastus")
            
            if not speech_key:
                return """Azure Speech Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… API Key

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Azure Speech:

1. Ø¨Ù‡ https://portal.azure.com Ø¨Ø±ÙˆÛŒØ¯
2. Cognitive Services > Speech Ø±Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯
3. API Key Ùˆ Region Ø±Ø§ Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯
4. Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set AZURE_SPEECH_KEY=your_key_here
   set AZURE_SPEECH_REGION=your_region_here

ÛŒØ§ Ø§Ø² Vosk Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ú©Ø§Ù…Ù„Ø§Ù‹ Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
            
            # ØªÙ†Ø¸ÛŒÙ… speech config
            speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
            speech_config.speech_recognition_language = "fa-IR"  # ÙØ§Ø±Ø³ÛŒ Ø§ÛŒØ±Ø§Ù†
            
            # ØªÙ†Ø¸ÛŒÙ… audio config
            audio_config = speechsdk.audio.AudioConfig(filename=audio_file)
            
            # Ø§ÛŒØ¬Ø§Ø¯ speech recognizer
            speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
            
            # ØªØ´Ø®ÛŒØµ
            result = speech_recognizer.recognize_once()
            
            if result.reason == speechsdk.ResultReason.RecognizedSpeech:
                return result.text
            elif result.reason == speechsdk.ResultReason.NoMatch:
                return "Azure Speech: ØªØ´Ø®ÛŒØµ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯"
            else:
                return f"Azure Speech Error: {result.reason}"
                
        except Exception as e:
            return f"Azure Speech Error: {str(e)}"
    
    def transcribe_with_assemblyai(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ AssemblyAI"""
        try:
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª AssemblyAI (Ù†ÛŒØ§Ø² Ø¨Ù‡ API Key)
            api_key = os.getenv("ASSEMBLYAI_API_KEY")
            
            if not api_key:
                return """AssemblyAI Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… API Key

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AssemblyAI:

1. Ø¨Ù‡ https://www.assemblyai.com Ø¨Ø±ÙˆÛŒØ¯
2. Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯
3. API Key Ø±Ø§ Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯
4. Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set ASSEMBLYAI_API_KEY=your_key_here

ÛŒØ§ Ø§Ø² Vosk Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ú©Ø§Ù…Ù„Ø§Ù‹ Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
            
            # ØªÙ†Ø¸ÛŒÙ… API key
            aai.settings.api_key = api_key
            
            # Ø§ÛŒØ¬Ø§Ø¯ transcriber
            transcriber = aai.Transcriber()
            
            # Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„
            transcript = transcriber.transcribe(audio_file)
            
            if transcript.status == aai.TranscriptStatus.completed:
                return transcript.text
            else:
                return f"AssemblyAI Error: {transcript.status}"
                
        except Exception as e:
            return f"AssemblyAI Error: {str(e)}"
    
    def transcribe_with_huggingface(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Hugging Face Transformers"""
        try:
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
            if self.model_name == "hf_wav2vec2_persian":
                # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ
                try:
                    model_name = "m3hrdadfi/wav2vec2-large-xlsr-53-persian"
                    processor = AutoProcessor.from_pretrained(model_name)
                    model = AutoModelForCTC.from_pretrained(model_name)
                except Exception as e:
                    # Ø§Ú¯Ø± Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø² Ù…Ø¯Ù„ Ø¹Ù…ÙˆÙ…ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
                    error_msg = str(e)
                    if "not a valid model identifier" in error_msg:
                        return f"""Hugging Face Error: Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª

Ù…Ø´Ú©Ù„: Ù…Ø¯Ù„ m3hrdadfi/wav2vec2-large-xlsr-53-persian ÛŒØ§ÙØª Ù†Ø´Ø¯

Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:
1. Ø§ØªØµØ§Ù„ Ø§ÛŒÙ†ØªØ±Ù†Øª Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯
2. Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:
   â€¢ Vosk Persian (Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ)
   â€¢ Whisper Medium/Large (Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡)
   â€¢ Wav2Vec2 Multilingual (Hugging Face)

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Hugging Face:
1. Ø¨Ù‡ https://huggingface.co Ø¨Ø±ÙˆÛŒØ¯
2. Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯
3. Ø§Ø² Ø¯Ø³ØªÙˆØ± Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:
   hf auth login
"""
                    else:
                        return f"Hugging Face Error: {error_msg}. Ù„Ø·ÙØ§Ù‹ Ø§Ø² Vosk Persian ÛŒØ§ Whisper Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯."
                    
            elif self.model_name == "hf_whisper_persian":
                # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Whisper ÙØ§Ø±Ø³ÛŒ
                try:
                    model_name = "m3hrdadfi/whisper-persian"
                    processor = AutoProcessor.from_pretrained(model_name)
                    model = AutoModelForCTC.from_pretrained(model_name)
                except Exception as e:
                    error_msg = str(e)
                    if "not a valid model identifier" in error_msg:
                        return f"""Hugging Face Error: Ù…Ø¯Ù„ Whisper ÙØ§Ø±Ø³ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª

Ù…Ø´Ú©Ù„: Ù…Ø¯Ù„ m3hrdadfi/whisper-persian ÛŒØ§ÙØª Ù†Ø´Ø¯

Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:
1. Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:
   â€¢ Whisper Medium/Large (Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡)
   â€¢ Vosk Persian (Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ)
   â€¢ Whisper Ø¹Ø§Ø¯ÛŒ (Hugging Face)

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Hugging Face:
1. Ø¨Ù‡ https://huggingface.co Ø¨Ø±ÙˆÛŒØ¯
2. Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯
3. Ø§Ø² Ø¯Ø³ØªÙˆØ± Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:
   hf auth login
"""
                    else:
                        return f"Hugging Face Error: {error_msg}. Ù„Ø·ÙØ§Ù‹ Ø§Ø² Whisper Ø¹Ø§Ø¯ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯."
                    
            elif self.model_name == "hf_wav2vec2_persian_alt":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø¹Ù…ÙˆÙ…ÛŒ wav2vec2
                try:
                    model_name = "facebook/wav2vec2-large-xlsr-53"
                    processor = AutoProcessor.from_pretrained(model_name)
                    model = AutoModelForCTC.from_pretrained(model_name)
                except Exception as e:
                    return f"Hugging Face Error: Ù…Ø¯Ù„ wav2vec2 Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª ({str(e)}). Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯."
            else:
                # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Whisper
                model_name = self.model_name.replace("hf_", "").replace("_hf", "")
                if model_name == "whisper_large":
                    model_name = "openai/whisper-large-v2"
                else:
                    model_name = f"openai/whisper-{model_name}"
                
                try:
                    processor = AutoProcessor.from_pretrained(model_name)
                    model = AutoModelForCTC.from_pretrained(model_name)
                except Exception as e:
                    return f"Hugging Face Error: Ù…Ø¯Ù„ {model_name} Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª ({str(e)}). Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯."
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            try:
                import librosa
                audio, sr = librosa.load(audio_file, sr=16000)
            except ImportError:
                try:
                    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² soundfile Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
                    import soundfile as sf
                    audio, sr = sf.read(audio_file)
                    if sr != 16000:
                        # ØªØ¨Ø¯ÛŒÙ„ sample rate Ø¨Ù‡ 16000
                        import numpy as np
                        from scipy import signal
                        audio = signal.resample(audio, int(len(audio) * 16000 / sr))
                        sr = 16000
                except ImportError:
                    return "Hugging Face Error: Neither librosa nor soundfile is available. Install with: pip install librosa soundfile"
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´
            try:
                inputs = processor(audio, sampling_rate=sr, return_tensors="pt")
                
                # ØªØ´Ø®ÛŒØµ
                with torch.no_grad():
                    logits = model(inputs.input_values).logits
                
                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ØªÙ†
                predicted_ids = torch.argmax(logits, dim=-1)
                text = processor.batch_decode(predicted_ids)[0]
                
                return text.strip()
            except Exception as e:
                return f"Hugging Face Error: Model processing failed - {str(e)}"
            
        except Exception as e:
            return f"Hugging Face Error: {str(e)}"
    
    def transcribe_with_speechrecognition(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ SpeechRecognition"""
        try:
            r = sr.Recognizer()
            
            with sr.AudioFile(audio_file) as source:
                audio = r.record(source)
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø³Ø±ÙˆÛŒØ³ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¯Ù„
            if self.model_name == "speechrecognition_google":
                text = r.recognize_google(audio, language="fa-IR")
            elif self.model_name == "speechrecognition_sphinx":
                try:
                    text = r.recognize_sphinx(audio)
                except Exception as e:
                    if "missing PocketSphinx module" in str(e):
                        return """SpeechRecognition Error: PocketSphinx not installed

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² CMU Sphinx:
1. PocketSphinx Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯:
   pip install PocketSphinx
2. ÛŒØ§ Ø§Ø² Google Speech Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø±Ø§ÛŒÚ¯Ø§Ù†)

CMU Sphinx ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
"""
                    else:
                        return f"SpeechRecognition Error: {str(e)}"
            elif self.model_name == "speechrecognition_wit":
                api_key = os.getenv("WIT_AI_KEY")
                if not api_key:
                    return """SpeechRecognition Error: Wit.ai API Key not found

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Wit.ai:
1. Ø¨Ù‡ https://wit.ai Ø¨Ø±ÙˆÛŒØ¯
2. Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯
3. API Key Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯
4. Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set WIT_AI_KEY=your_key_here

ÛŒØ§ Ø§Ø² Google Speech Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
                text = r.recognize_wit(audio, key=api_key)
            elif self.model_name == "speechrecognition_azure":
                api_key = os.getenv("AZURE_SPEECH_KEY")
                region = os.getenv("AZURE_SPEECH_REGION")
                if not api_key or not region:
                    return """SpeechRecognition Error: Azure Speech API Key not found

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Azure Speech:
1. Ø¨Ù‡ https://portal.azure.com Ø¨Ø±ÙˆÛŒØ¯
2. Cognitive Services > Speech Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯
3. API Key Ùˆ Region Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯
4. Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set AZURE_SPEECH_KEY=your_key_here
   set AZURE_SPEECH_REGION=your_region_here

ÛŒØ§ Ø§Ø² Google Speech Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
                text = r.recognize_azure(audio, key=api_key, location=region)
            elif self.model_name == "speechrecognition_bing":
                api_key = os.getenv("BING_KEY")
                if not api_key:
                    return """SpeechRecognition Error: Bing Speech API Key not found

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Bing Speech:
1. Ø¨Ù‡ https://azure.microsoft.com Ø¨Ø±ÙˆÛŒØ¯
2. Bing Speech API ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
3. API Key Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯
4. Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set BING_KEY=your_key_here

ÛŒØ§ Ø§Ø² Google Speech Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
                text = r.recognize_bing(audio, key=api_key)
            elif self.model_name == "speechrecognition_houndify":
                client_id = os.getenv("HOUNDIFY_CLIENT_ID")
                client_key = os.getenv("HOUNDIFY_CLIENT_KEY")
                if not client_id or not client_key:
                    return """SpeechRecognition Error: Houndify API Keys not found

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Houndify:
1. Ø¨Ù‡ https://www.houndify.com Ø¨Ø±ÙˆÛŒØ¯
2. Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯
3. Client ID Ùˆ Client Key Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯
4. Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set HOUNDIFY_CLIENT_ID=your_client_id
   set HOUNDIFY_CLIENT_KEY=your_client_key

ÛŒØ§ Ø§Ø² Google Speech Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
                text = r.recognize_houndify(audio, client_id=client_id, client_key=client_key)
            elif self.model_name == "speechrecognition_ibm":
                username = os.getenv("IBM_USERNAME")
                password = os.getenv("IBM_PASSWORD")
                if not username or not password:
                    return """SpeechRecognition Error: IBM Speech API Credentials not found

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² IBM Speech:
1. Ø¨Ù‡ https://www.ibm.com/cloud/watson-speech-to-text Ø¨Ø±ÙˆÛŒØ¯
2. Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯
3. Username Ùˆ Password Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯
4. Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set IBM_USERNAME=your_username
   set IBM_PASSWORD=your_password

ÛŒØ§ Ø§Ø² Google Speech Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
                text = r.recognize_ibm(audio, username=username, password=password)
            else:
                return "SpeechRecognition Error: Unknown service"
            
            return text.strip()
            
        except Exception as e:
            error_msg = str(e)
            if "key must be a string" in error_msg:
                return f"""SpeechRecognition Error: API Key Ù…Ø´Ú©Ù„ Ø¯Ø§Ø±Ø¯

Ù…Ø´Ú©Ù„: Ú©Ù„ÛŒØ¯ API Ø¨Ù‡ Ø¯Ø±Ø³ØªÛŒ ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª

Ø±Ø§Ù‡â€ŒØ­Ù„â€ŒÙ‡Ø§:
1. Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯
2. API Key Ø±Ø§ Ø¯ÙˆØ¨Ø§Ø±Ù‡ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯
3. Ø§Ø² Google Speech Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø±Ø§ÛŒÚ¯Ø§Ù†)

Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ:
set API_KEY_NAME=your_key_here
"""
            else:
                return f"SpeechRecognition Error: {error_msg}"
    
    def transcribe_with_silero(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Silero STT"""
        try:
            import torch
            import torchaudio
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
            if self.model_name == "silero_stt_en":
                model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en')
            else:  # silero_stt_multilingual
                model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='multilingual')
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            audio, sample_rate = torchaudio.load(audio_file)
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ mono
            if audio.shape[0] > 1:
                audio = torch.mean(audio, dim=0, keepdim=True)
            
            # ØªØ´Ø®ÛŒØµ
            text = decoder(model(audio[0]))
            
            return text.strip()
            
        except ImportError as e:
            return f"Silero STT Error: Required dependencies not installed. Install with: pip install torch torchaudio"
        except Exception as e:
            return f"Silero STT Error: {str(e)}"
    
    def transcribe_with_kaldi(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Kaldi"""
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Kaldi
            if not KALDI_AVAILABLE:
                return "Kaldi Error: kaldi-io not installed. Install with: pip install kaldi-io"
            
            # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Kaldi Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯
            if self.model_name == "kaldi_persian":
                # Ø¨Ø±Ø§ÛŒ Kaldi PersianØŒ Ø§Ø² ÛŒÚ© Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                # Ø§ÛŒÙ† ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Kaldi Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´ÙˆØ¯
                
                # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
                import soundfile as sf
                audio, sample_rate = sf.read(audio_file)
                
                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ mono Ø§Ú¯Ø± stereo Ø¨Ø§Ø´Ø¯
                if len(audio.shape) > 1:
                    audio = audio.mean(axis=1)
                
                # ØªØ¨Ø¯ÛŒÙ„ sample rate Ø¨Ù‡ 16000
                if sample_rate != 16000:
                    from scipy import signal
                    audio = signal.resample(audio, int(len(audio) * 16000 / sample_rate))
                    sample_rate = 16000
                
                # Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Kaldi
                # Ø§ÛŒÙ† ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ø¯ Ø¨Ø§ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÙˆØ§Ù‚Ø¹ÛŒ Kaldi Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø´ÙˆØ¯
                
                # Ø¨Ø±Ø§ÛŒ Ø­Ø§Ù„ Ø­Ø§Ø¶Ø±ØŒ Ø§Ø² ÛŒÚ© Ù…Ø¯Ù„ Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
                # Ø¯Ø± Ø¢ÛŒÙ†Ø¯Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Kaldi ÙˆØ§Ù‚Ø¹ÛŒ Ø±Ø§ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø·ÙˆÙ„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
                duration = len(audio) / sample_rate
                
                # Ù¾ÛŒØ§Ù… Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§ÛŒÙ„
                result = f"Kaldi Persian: ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯ (Ù…Ø¯Øª: {duration:.2f} Ø«Ø§Ù†ÛŒÙ‡)\n"
                result += "Ù…Ø¯Ù„ Kaldi ÙØ§Ø±Ø³ÛŒ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡ Ø§Ø³Øª.\n"
                result += "Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒØŒ Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:\n"
                result += "â€¢ Vosk Persian (Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ)\n"
                result += "â€¢ Whisper Medium/Large (Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡)\n"
                result += "â€¢ Wav2Vec2 Persian (Hugging Face)"
                
                return result
                
            elif self.model_name == "kaldi_english":
                # Ø¨Ø±Ø§ÛŒ Kaldi English
                import soundfile as sf
                audio, sample_rate = sf.read(audio_file)
                
                if len(audio.shape) > 1:
                    audio = audio.mean(axis=1)
                
                if sample_rate != 16000:
                    from scipy import signal
                    audio = signal.resample(audio, int(len(audio) * 16000 / sample_rate))
                    sample_rate = 16000
                
                # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø·ÙˆÙ„ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
                duration = len(audio) / sample_rate
                
                # Ù¾ÛŒØ§Ù… Ù†Ù…ÙˆÙ†Ù‡ Ø¨Ø§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª ÙØ§ÛŒÙ„
                result = f"Kaldi English: ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯ (Ù…Ø¯Øª: {duration:.2f} Ø«Ø§Ù†ÛŒÙ‡)\n"
                result += "Ù…Ø¯Ù„ Kaldi Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¯Ø± Ø­Ø§Ù„ ØªÙˆØ³Ø¹Ù‡ Ø§Ø³Øª.\n"
                result += "Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒØŒ Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ú¯Ø²ÛŒÙ†Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯:\n"
                result += "â€¢ Vosk Small/Large (Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ø±Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)\n"
                result += "â€¢ Whisper (Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡)\n"
                result += "â€¢ Silero STT English"
                
                return result
            else:
                return "Kaldi Error: Unknown Kaldi model"
            
        except ImportError as e:
            return f"Kaldi Error: Required dependencies not installed. Install with: pip install kaldi-io soundfile scipy"
        except Exception as e:
            return f"Kaldi Error: {str(e)}"
    
    def transcribe_with_iranian(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¨ÙˆÙ…ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ"""
        try:
            # Ø§ÛŒÙ† ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª
            # Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ API key Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø§Ø³Øª
            
            if self.model_name == "iranian_arvan":
                return "Arvan Cloud Speech Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… API Key\nØ¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Arvan Cloud Speech API key Ø®ÙˆØ¯ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯."
            elif self.model_name == "iranian_fanap":
                return "Fanap Speech Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… API Key\nØ¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Fanap Speech API key Ø®ÙˆØ¯ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯."
            elif self.model_name == "iranian_parsijoo":
                return "Parsijoo Speech Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… API Key\nØ¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Parsijoo Speech API key Ø®ÙˆØ¯ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯."
            else:
                return "Iranian Service Error: Unknown service"
            
        except Exception as e:
            return f"Iranian Service Error: {str(e)}"


class VoiceApp(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Voice Analyzer")
        self.resize(900, 600)  # Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ú©Ø±Ø¯Ù† Ø³Ø§ÛŒØ² Ø§Ù¾
        self.layout = QVBoxLayout(self)

        self.label = QLabel("Select an audio file:")
        self.layout.addWidget(self.label)
        
        self.model_label = QLabel("Selected Model: vosk_persian (default)")
        self.model_label.setStyleSheet("color: blue; font-weight: bold;")
        self.layout.addWidget(self.model_label)

        # Ø¯Ú©Ù…Ù‡ Ù‡Ø§
        self.btn_select = QPushButton("Choose File")
        self.btn_select.setMinimumHeight(40)
        self.btn_select.clicked.connect(self.select_file)
        self.layout.addWidget(self.btn_select)

        self.btn_start_pause = QPushButton("Start / Pause")
        self.btn_start_pause.setMinimumHeight(40)
        self.btn_start_pause.clicked.connect(self.toggle_start_pause)
        self.layout.addWidget(self.btn_start_pause)

        self.btn_stop = QPushButton("Stop")
        self.btn_stop.setMinimumHeight(40)
        self.btn_stop.clicked.connect(self.stop_processing)
        self.layout.addWidget(self.btn_stop)

        self.btn_open_text = QPushButton("Open Text")
        self.btn_open_text.setMinimumHeight(40)
        self.btn_open_text.clicked.connect(self.open_text_file)
        self.layout.addWidget(self.btn_open_text)

        self.btn_manage_dict = QPushButton("Manage Dictionary")
        self.btn_manage_dict.setMinimumHeight(40)
        self.btn_manage_dict.clicked.connect(self.open_dict_manager)
        self.layout.addWidget(self.btn_manage_dict)

        self.btn_google_setup = QPushButton("Google Setup Guide")
        self.btn_google_setup.setMinimumHeight(40)
        self.btn_google_setup.setStyleSheet("background-color: #4285f4; color: white;")
        self.btn_google_setup.clicked.connect(self.show_google_setup_guide)
        self.layout.addWidget(self.btn_google_setup)

        self.btn_download_models = QPushButton("Download Models")
        self.btn_download_models.setMinimumHeight(40)
        self.btn_download_models.setStyleSheet("background-color: #ff6b35; color: white;")
        self.btn_download_models.clicked.connect(self.show_download_models_dialog)
        self.layout.addWidget(self.btn_download_models)

        self.btn_change_model = QPushButton("Change Model")
        self.btn_change_model.setMinimumHeight(40)
        self.btn_change_model.setStyleSheet("background-color: #9c27b0; color: white;")
        self.btn_change_model.clicked.connect(self.change_model)
        self.layout.addWidget(self.btn_change_model)

        self.btn_huggingface_setup = QPushButton("Hugging Face Setup")
        self.btn_huggingface_setup.setMinimumHeight(40)
        self.btn_huggingface_setup.setStyleSheet("background-color: #ff6b35; color: white;")
        self.btn_huggingface_setup.clicked.connect(self.show_huggingface_setup_guide)
        self.layout.addWidget(self.btn_huggingface_setup)

        self.btn_speechrecognition_setup = QPushButton("SpeechRecognition Setup")
        self.btn_speechrecognition_setup.setMinimumHeight(40)
        self.btn_speechrecognition_setup.setStyleSheet("background-color: #2196F3; color: white;")
        self.btn_speechrecognition_setup.clicked.connect(self.show_speechrecognition_setup_guide)
        self.layout.addWidget(self.btn_speechrecognition_setup)

        self.progress = QProgressBar()
        self.layout.addWidget(self.progress)
        
        # Progress bar Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯
        self.download_progress = QProgressBar()
        self.download_progress.setVisible(False)
        self.download_progress.setStyleSheet("""
            QProgressBar {
                border: 2px solid grey;
                border-radius: 5px;
                text-align: center;
                font-weight: bold;
                font-size: 12px;
            }
            QProgressBar::chunk {
                background-color: #4CAF50;
                border-radius: 3px;
            }
        """)
        self.layout.addWidget(self.download_progress)
        
        # Status label Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯
        self.download_status = QLabel("")
        self.download_status.setVisible(False)
        self.download_status.setStyleSheet("""
            color: #2196F3; 
            font-weight: bold; 
            font-size: 14px;
            padding: 5px;
            background-color: #E3F2FD;
            border: 1px solid #2196F3;
            border-radius: 5px;
        """)
        self.download_status.setAlignment(Qt.AlignCenter)
        self.layout.addWidget(self.download_status)

        self.scroll = QScrollArea()
        self.text_edit = QTextEdit()
        self.text_edit.setReadOnly(True)
        self.text_edit.setFontPointSize(14)  # Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ø´Ø¯Ù† Ù…ØªÙ† Ù†Ù…Ø§ÛŒØ´ÛŒ
        self.scroll.setWidget(self.text_edit)
        self.scroll.setWidgetResizable(True)
        self.layout.addWidget(self.scroll)

        self.audio_path = None
        self.output_file = None
        self.thread = None
        self.dict_manager_window = None
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ù…Ø¯Ù„ Ø¢Ø®Ø±
        config = ConfigManager.load_config()
        self.selected_model = config.get("selected_model", "vosk_persian")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
        last_audio_path = config.get("last_audio_path", "")
        if last_audio_path and os.path.exists(last_audio_path):
            self.audio_path = last_audio_path
            self.label.setText(f"Last File: {last_audio_path}")
        
        # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØª Ùˆ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡
        window_geometry = config.get("window_geometry")
        if window_geometry:
            self.setGeometry(
                window_geometry.get("x", 100),
                window_geometry.get("y", 100),
                window_geometry.get("width", 900),
                window_geometry.get("height", 600)
            )
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„
        self.update_model_display()

    def update_model_display(self):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡"""
        model_info = ModelDownloader.DOWNLOADABLE_MODELS.get(self.selected_model, {})
        if model_info:
            model_name = model_info.get("name", self.selected_model)
            model_type = model_info.get("type", "Unknown")
            model_language = model_info.get("language", "Unknown")
            self.model_label.setText(f"Selected Model: {self.selected_model} ({model_type} - {model_language})")
        else:
            self.model_label.setText(f"Selected Model: {self.selected_model}")

    def select_file(self):
        file_path, _ = QFileDialog.getOpenFileName(
            self, "Select Audio File", "", 
            "Audio Files (*.mp3 *.wav *.m4a *.ogg *.flac)"
        )
        if file_path:
            self.audio_path = file_path
            self.label.setText(f"Selected: {file_path}")
            self.text_edit.clear()
            self.output_file = None
            self.thread = None  # Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† thread Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø³ÛŒØ± Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            ConfigManager.update_audio_path(file_path)

    def start_transcription(self):
        if not self.audio_path:
            QMessageBox.warning(self, "Warning", "No audio file selected!")
            return

        # Ù†Ù…Ø§ÛŒØ´ dialog Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„
        dialog = ModelSelectionDialog(self)
        if dialog.exec() == QDialog.Accepted:
            self.selected_model = dialog.get_selected_model()
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
            ConfigManager.update_model(self.selected_model)
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„
            self.update_model_display()
            
            # Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ
            if self.selected_model in ["whisper_tiny", "whisper_base"]:
                reply = QMessageBox.question(
                    self, "Ù‡Ø´Ø¯Ø§Ø±", 
                    f"Ù…Ø¯Ù„ {self.selected_model} Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¶Ø¹ÛŒÙ Ø§Ø³Øª Ùˆ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù†ØªØ§ÛŒØ¬ Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¨Ø¯Ù‡Ø¯.\nØ¢ÛŒØ§ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒØ¯ØŸ",
                    QMessageBox.Yes | QMessageBox.No,
                    QMessageBox.No
                )
                if reply == QMessageBox.No:
                    return  # Ú©Ø§Ø±Ø¨Ø± Ø§Ù†ØµØ±Ø§Ù Ø¯Ø§Ø¯
            
            # Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Vosk
            if self.selected_model in ["vosk_small", "vosk_large"]:
                reply = QMessageBox.question(
                    self, "Ù‡Ø´Ø¯Ø§Ø± Ø²Ø¨Ø§Ù†", 
                    f"Ù…Ø¯Ù„ {self.selected_model} ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ùˆ ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯!\n\nØ¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø² 'Vosk Persian' Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\nØ¢ÛŒØ§ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒØ¯ØŸ",
                    QMessageBox.Yes | QMessageBox.No,
                    QMessageBox.No
                )
                if reply == QMessageBox.No:
                    return  # Ú©Ø§Ø±Ø¨Ø± Ø§Ù†ØµØ±Ø§Ù Ø¯Ø§Ø¯
            
            # Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÙˆÙ„ÛŒ
            paid_models = [
                "google_enhanced", "google_phone_call", "google_medical", "google_video",
                "azure_enhanced", "assemblyai_enhanced"
            ]
            if self.selected_model in paid_models:
                reply = QMessageBox.question(
                    self, "Ù‡Ø´Ø¯Ø§Ø± Ù‡Ø²ÛŒÙ†Ù‡", 
                    f"Ù…Ø¯Ù„ {self.selected_model} Ù¾ÙˆÙ„ÛŒ Ø§Ø³Øª Ùˆ Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\nØ¢ÛŒØ§ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒØ¯ØŸ",
                    QMessageBox.Yes | QMessageBox.No,
                    QMessageBox.No
                )
                if reply == QMessageBox.No:
                    return  # Ú©Ø§Ø±Ø¨Ø± Ø§Ù†ØµØ±Ø§Ù Ø¯Ø§Ø¯
        else:
            return  # Ú©Ø§Ø±Ø¨Ø± cancel Ú©Ø±Ø¯

        self.text_edit.clear()
        self.output_file = None
        self.thread = TranscribeThread(self.audio_path, self.selected_model)
        self.thread.progress.connect(self.progress.setValue)
        self.thread.finished.connect(self.display_result)
        self.thread.download_progress.connect(self.update_download_progress)
        self.thread.download_status.connect(self.update_download_status)
        self.thread.start()

    def update_download_progress(self, percent):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ progress bar Ø¯Ø§Ù†Ù„ÙˆØ¯"""
        self.download_progress.setValue(percent)
        self.download_progress.setFormat(f"Ø¯Ø§Ù†Ù„ÙˆØ¯: {percent}%")
        
        if percent == 100:
            # Ú©Ù…ÛŒ ØµØ¨Ø± Ú©Ù†ÛŒØ¯ ØªØ§ Ú©Ø§Ø±Ø¨Ø± progress Ú©Ø§Ù…Ù„ Ø±Ø§ Ø¨Ø¨ÛŒÙ†Ø¯
            import time
            time.sleep(1)
            self.download_progress.setVisible(False)
            self.download_status.setVisible(False)
    
    def update_download_status(self, message):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ status Ø¯Ø§Ù†Ù„ÙˆØ¯"""
        self.download_status.setText(f"ğŸ“¥ {message}")
        self.download_status.setVisible(True)
        self.download_progress.setVisible(True)
        self.download_progress.setValue(0)  # Ø´Ø±ÙˆØ¹ Ø§Ø² ØµÙØ±

    def display_result(self, text):
        self.text_edit.setPlainText(text)
        save_path, _ = QFileDialog.getSaveFileName(
            self, "Save Text", "", "Text Files (*.txt)"
        )
        if save_path:
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(text)
            self.output_file = save_path

    def open_text_file(self):
        if self.output_file and os.path.exists(self.output_file):
            webbrowser.open(self.output_file)
        else:
            QMessageBox.warning(self, "Warning", "No saved text file to open!")

    def toggle_start_pause(self):
        if not self.audio_path:
            QMessageBox.warning(self, "Warning", "No audio file selected!")
            return

        if not self.thread:
            self.start_transcription()
        else:
            QMessageBox.information(self, "Info", "Pause / Resume toggle clicked (future implementation).")

    def stop_processing(self):
        if self.thread and self.thread.isRunning():
            QMessageBox.information(self, "Info", "Stop clicked (future implementation).")

    def open_dict_manager(self):
        if not self.dict_manager_window:
            self.dict_manager_window = DictManager()
        self.dict_manager_window.show()
        self.dict_manager_window.raise_()
        self.dict_manager_window.activateWindow()

    def change_model(self):
        """ØªØºÛŒÛŒØ± Ù…Ø¯Ù„ Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ¹ transcription"""
        dialog = ModelSelectionDialog(self)
        if dialog.exec() == QDialog.Accepted:
            self.selected_model = dialog.get_selected_model()
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
            ConfigManager.update_model(self.selected_model)
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„
            self.update_model_display()
            
            QMessageBox.information(self, "Model Changed", f"Model changed to: {self.selected_model}")

    def closeEvent(self, event):
        """Ø°Ø®ÛŒØ±Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù‡Ù†Ú¯Ø§Ù… Ø¨Ø³ØªÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡"""
        # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª Ùˆ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡
        geometry = {
            "x": self.x(),
            "y": self.y(),
            "width": self.width(),
            "height": self.height()
        }
        ConfigManager.update_window_geometry(geometry)
        event.accept()

    def show_google_setup_guide(self):
        from PySide6.QtWidgets import QDialog, QVBoxLayout, QTextEdit, QPushButton, QHBoxLayout
        from PySide6.QtCore import Qt
        from PySide6.QtGui import QTextCursor
        
        dialog = QDialog(self)
        dialog.setWindowTitle("Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Google Speech")
        dialog.setModal(True)
        dialog.resize(600, 500)
        
        layout = QVBoxLayout(dialog)
        
        # Ù…ØªÙ† Ø±Ø§Ù‡Ù†Ù…Ø§
        text_edit = QTextEdit()
        text_edit.setReadOnly(True)
        text_edit.setHtml("""
        <h2>Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Google Speech-to-Text</h2>
        
        <h3>ğŸ”§ Ù…Ø±Ø§Ø­Ù„ ØªÙ†Ø¸ÛŒÙ…:</h3>
        
        <h4>1ï¸âƒ£ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±ÙˆÚ˜Ù‡ Google Cloud:</h4>
        <p>â€¢ Ø¨Ù‡ <a href="https://console.cloud.google.com">Google Cloud Console</a> Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ Ø±ÙˆÛŒ "New Project" Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ù†Ø§Ù… Ù¾Ø±ÙˆÚ˜Ù‡ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ Ùˆ "Create" Ø¨Ø²Ù†ÛŒØ¯</p>
        
        <h4>2ï¸âƒ£ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ API:</h4>
        <p>â€¢ Ø¯Ø± Ù…Ù†ÙˆØŒ "APIs & Services" > "Library" Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ "Speech-to-Text API" Ø±Ø§ Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ø±ÙˆÛŒ Ø¢Ù† Ú©Ù„ÛŒÚ© Ú©Ø±Ø¯Ù‡ Ùˆ "Enable" Ø¨Ø²Ù†ÛŒØ¯</p>
        
        <h4>3ï¸âƒ£ Ø§ÛŒØ¬Ø§Ø¯ Service Account:</h4>
        <p>â€¢ "APIs & Services" > "Credentials" Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ "Create Credentials" > "Service Account" Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ù†Ø§Ù… Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Role: "Cloud Speech Client" Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯</p>
        
        <h4>4ï¸âƒ£ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù„ÛŒØ¯ JSON:</h4>
        <p>â€¢ Ø±ÙˆÛŒ Service Account Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ ØªØ¨ "Keys" > "Add Key" > "Create new key"</p>
        <p>â€¢ JSON Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯</p>
        
        <h4>5ï¸âƒ£ ØªÙ†Ø¸ÛŒÙ… Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ:</h4>
        <p>â€¢ ÙØ§ÛŒÙ„ JSON Ø±Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø§Ù…Ù† Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯</p>
        <p>â€¢ Ø¯Ø± Command Prompt Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:</p>
        <p><code>set GOOGLE_APPLICATION_CREDENTIALS=C:\\path\\to\\your\\key.json</code></p>
        
        <h3>ğŸ’¡ Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</h3>
        <p>â€¢ Google Speech 60 Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¯Ø± Ù…Ø§Ù‡ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø³Øª</p>
        <p>â€¢ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨ÛŒØ´ØªØ±ØŒ Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯</p>
        <p>â€¢ Whisper Ú©Ø§Ù…Ù„Ø§Ù‹ Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ø¢ÙÙ„Ø§ÛŒÙ† Ø§Ø³Øª</p>
        
        <h3>ğŸ”— Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÛŒØ¯:</h3>
        <p>â€¢ <a href="https://console.cloud.google.com">Google Cloud Console</a></p>
        <p>â€¢ <a href="https://cloud.google.com/speech-to-text/docs">Ù…Ø³ØªÙ†Ø¯Ø§Øª Speech-to-Text</a></p>
        <p>â€¢ <a href="https://cloud.google.com/docs/authentication/external/set-up-adc">Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Authentication</a></p>
        """)
        
        layout.addWidget(text_edit)
        
        # Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
        button_layout = QHBoxLayout()
        
        open_console_btn = QPushButton("Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Google Cloud Console")
        open_console_btn.setStyleSheet("background-color: #4285f4; color: white; padding: 8px;")
        open_console_btn.clicked.connect(lambda: webbrowser.open("https://console.cloud.google.com"))
        
        open_docs_btn = QPushButton("Ù…Ø³ØªÙ†Ø¯Ø§Øª Speech-to-Text")
        open_docs_btn.setStyleSheet("background-color: #34a853; color: white; padding: 8px;")
        open_docs_btn.clicked.connect(lambda: webbrowser.open("https://cloud.google.com/speech-to-text/docs"))
        
        close_btn = QPushButton("Ø¨Ø³ØªÙ†")
        close_btn.clicked.connect(dialog.accept)
        
        button_layout.addWidget(open_console_btn)
        button_layout.addWidget(open_docs_btn)
        button_layout.addWidget(close_btn)
        
        layout.addLayout(button_layout)
        
        dialog.exec()
    
    def show_download_models_dialog(self):
        """Ù†Ù…Ø§ÛŒØ´ dialog Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        from PySide6.QtWidgets import QDialog, QVBoxLayout, QListWidget, QPushButton, QHBoxLayout, QLabel, QTabWidget, QWidget
        from PySide6.QtCore import Qt
        
        dialog = QDialog(self)
        dialog.setWindowTitle("Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Speech-to-Text")
        dialog.setModal(True)
        dialog.resize(700, 500)
        
        layout = QVBoxLayout(dialog)
        
        # ØªÙˆØ¶ÛŒØ­Ø§Øª
        info_label = QLabel("Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± (Ú©Ø§Ù…Ù„Ø§Ù‹ Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ø¢ÙÙ„Ø§ÛŒÙ†):")
        layout.addWidget(info_label)
        
        # Tab widget Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        tab_widget = QTabWidget()
        
        # Tab Vosk
        vosk_tab = QWidget()
        vosk_layout = QVBoxLayout(vosk_tab)
        vosk_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "Vosk":
                status = "âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡" if ModelDownloader.is_model_downloaded(model_id) else "âŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                vosk_list.addItem(item_text)
        
        vosk_layout.addWidget(vosk_list)
        tab_widget.addTab(vosk_tab, "Vosk Models")
        
        # Tab Whisper
        whisper_tab = QWidget()
        whisper_layout = QVBoxLayout(whisper_tab)
        whisper_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "Whisper":
                status = "âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡" if ModelDownloader.is_model_downloaded(model_id) else "âŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                whisper_list.addItem(item_text)
        
        whisper_layout.addWidget(whisper_list)
        tab_widget.addTab(whisper_tab, "Whisper Models")
        
        # Tab Hugging Face
        hf_tab = QWidget()
        hf_layout = QVBoxLayout(hf_tab)
        hf_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "HuggingFace":
                status = "âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡" if ModelDownloader.is_model_downloaded(model_id) else "âŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                hf_list.addItem(item_text)
        
        hf_layout.addWidget(hf_list)
        tab_widget.addTab(hf_tab, "Hugging Face Models")
        
        # Tab SpeechRecognition
        sr_tab = QWidget()
        sr_layout = QVBoxLayout(sr_tab)
        sr_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "SpeechRecognition":
                status = "âœ… Ø¢Ù…Ø§Ø¯Ù‡" if True else "âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                sr_list.addItem(item_text)
        
        sr_layout.addWidget(sr_list)
        tab_widget.addTab(sr_tab, "SpeechRecognition")
        
        # Tab Silero
        silero_tab = QWidget()
        silero_layout = QVBoxLayout(silero_tab)
        silero_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "Silero":
                status = "âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡" if ModelDownloader.is_model_downloaded(model_id) else "âŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                silero_list.addItem(item_text)
        
        silero_layout.addWidget(silero_list)
        tab_widget.addTab(silero_tab, "Silero STT")
        
        # Tab Kaldi
        kaldi_tab = QWidget()
        kaldi_layout = QVBoxLayout(kaldi_tab)
        kaldi_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "Kaldi":
                status = "âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡" if ModelDownloader.is_model_downloaded(model_id) else "âŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                kaldi_list.addItem(item_text)
        
        kaldi_layout.addWidget(kaldi_list)
        tab_widget.addTab(kaldi_tab, "Kaldi Models")
        
        # Tab Iranian Services
        iranian_tab = QWidget()
        iranian_layout = QVBoxLayout(iranian_tab)
        iranian_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "Iranian":
                status = "âœ… Ø¢Ù…Ø§Ø¯Ù‡" if True else "âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                iranian_list.addItem(item_text)
        
        iranian_layout.addWidget(iranian_list)
        tab_widget.addTab(iranian_tab, "Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ")
        
        layout.addWidget(tab_widget)
        
        # Ø°Ø®ÛŒØ±Ù‡ reference Ù‡Ø§
        dialog.vosk_list = vosk_list
        dialog.whisper_list = whisper_list
        dialog.hf_list = hf_list
        dialog.sr_list = sr_list
        dialog.silero_list = silero_list
        dialog.kaldi_list = kaldi_list
        dialog.iranian_list = iranian_list
        dialog.tab_widget = tab_widget
        
        # Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
        button_layout = QHBoxLayout()
        
        download_btn = QPushButton("Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡")
        download_btn.setStyleSheet("background-color: #4CAF50; color: white; padding: 8px;")
        download_btn.clicked.connect(lambda: self.download_selected_model(dialog))
        
        download_all_btn = QPushButton("Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk")
        download_all_btn.setStyleSheet("background-color: #2196F3; color: white; padding: 8px;")
        download_all_btn.clicked.connect(lambda: self.download_all_vosk_models(dialog))
        
        download_whisper_btn = QPushButton("Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper")
        download_whisper_btn.setStyleSheet("background-color: #FF9800; color: white; padding: 8px;")
        download_whisper_btn.clicked.connect(lambda: self.download_all_whisper_models(dialog))
        
        close_btn = QPushButton("Ø¨Ø³ØªÙ†")
        close_btn.clicked.connect(dialog.accept)
        
        button_layout.addWidget(download_btn)
        button_layout.addWidget(download_all_btn)
        button_layout.addWidget(download_whisper_btn)
        button_layout.addWidget(close_btn)
        
        layout.addLayout(button_layout)
        
        dialog.exec()
    
    def show_huggingface_setup_guide(self):
        """Ù†Ù…Ø§ÛŒØ´ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Hugging Face"""
        from PySide6.QtWidgets import QDialog, QVBoxLayout, QTextEdit, QPushButton, QHBoxLayout
        from PySide6.QtCore import Qt
        
        dialog = QDialog(self)
        dialog.setWindowTitle("Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Hugging Face")
        dialog.setModal(True)
        dialog.resize(600, 500)
        
        layout = QVBoxLayout(dialog)
        
        # Ù…ØªÙ† Ø±Ø§Ù‡Ù†Ù…Ø§
        text_edit = QTextEdit()
        text_edit.setReadOnly(True)
        text_edit.setHtml("""
        <h2>Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Hugging Face</h2>
        
        <h3>ğŸ”§ Ù…Ø±Ø§Ø­Ù„ ØªÙ†Ø¸ÛŒÙ…:</h3>
        
        <h4>1ï¸âƒ£ Ø§ÛŒØ¬Ø§Ø¯ Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ:</h4>
        <p>â€¢ Ø¨Ù‡ <a href="https://huggingface.co">Hugging Face</a> Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ Ø±ÙˆÛŒ "Sign Up" Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø®ÙˆØ¯ Ø±Ø§ Ø¨Ø³Ø§Ø²ÛŒØ¯</p>
        
        <h4>2ï¸âƒ£ Ù†ØµØ¨ Hugging Face CLI:</h4>
        <p>â€¢ Ø¯Ø± Command Prompt Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:</p>
        <p><code>pip install huggingface_hub</code></p>
        
        <h4>3ï¸âƒ£ ÙˆØ±ÙˆØ¯ Ø¨Ù‡ Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ:</h4>
        <p>â€¢ Ø¯Ø± Command Prompt Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:</p>
        <p><code>huggingface-cli login</code></p>
        <p>â€¢ Token Ø®ÙˆØ¯ Ø±Ø§ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯</p>
        
        <h4>4ï¸âƒ£ Ø¯Ø±ÛŒØ§ÙØª Token:</h4>
        <p>â€¢ Ø¨Ù‡ <a href="https://huggingface.co/settings/tokens">Settings > Tokens</a> Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ "New token" Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ù†Ø§Ù… Ùˆ Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ‡Ø§ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Token Ø±Ø§ Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯</p>
        
        <h3>ğŸ’¡ Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</h3>
        <p>â€¢ Ø¨Ø±Ø®ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø­Ø±Ø§Ø² Ù‡ÙˆÛŒØª Ø¯Ø§Ø±Ù†Ø¯</p>
        <p>â€¢ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†Ø¨Ø§Ø´Ù†Ø¯</p>
        <p>â€¢ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯</p>
        
        <h3>ğŸ”— Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÛŒØ¯:</h3>
        <p>â€¢ <a href="https://huggingface.co">Hugging Face</a></p>
        <p>â€¢ <a href="https://huggingface.co/models">Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯</a></p>
        <p>â€¢ <a href="https://huggingface.co/docs/hub/quick-start">Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø³Ø±ÛŒØ¹</a></p>
        """)
        
        layout.addWidget(text_edit)
        
        # Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
        button_layout = QHBoxLayout()
        
        open_hf_btn = QPushButton("Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Hugging Face")
        open_hf_btn.setStyleSheet("background-color: #ff6b35; color: white; padding: 8px;")
        open_hf_btn.clicked.connect(lambda: webbrowser.open("https://huggingface.co"))
        
        open_tokens_btn = QPushButton("Ù…Ø¯ÛŒØ±ÛŒØª Tokens")
        open_tokens_btn.setStyleSheet("background-color: #4CAF50; color: white; padding: 8px;")
        open_tokens_btn.clicked.connect(lambda: webbrowser.open("https://huggingface.co/settings/tokens"))
        
        close_btn = QPushButton("Ø¨Ø³ØªÙ†")
        close_btn.clicked.connect(dialog.accept)
        
        button_layout.addWidget(open_hf_btn)
        button_layout.addWidget(open_tokens_btn)
        button_layout.addWidget(close_btn)
        
        layout.addLayout(button_layout)
        
        dialog.exec()
    
    def show_speechrecognition_setup_guide(self):
        """Ù†Ù…Ø§ÛŒØ´ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… SpeechRecognition"""
        from PySide6.QtWidgets import QDialog, QVBoxLayout, QTextEdit, QPushButton, QHBoxLayout
        from PySide6.QtCore import Qt
        
        dialog = QDialog(self)
        dialog.setWindowTitle("Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… SpeechRecognition")
        dialog.setModal(True)
        dialog.resize(700, 600)
        
        layout = QVBoxLayout(dialog)
        
        # Ù…ØªÙ† Ø±Ø§Ù‡Ù†Ù…Ø§
        text_edit = QTextEdit()
        text_edit.setReadOnly(True)
        text_edit.setHtml("""
        <h2>Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… SpeechRecognition</h2>
        
        <h3>ğŸ”§ ØªÙ†Ø¸ÛŒÙ… API Keys:</h3>
        
        <h4>1ï¸âƒ£ Google Speech (Ø±Ø§ÛŒÚ¯Ø§Ù† - Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ):</h4>
        <p>â€¢ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ API Key Ù†Ø¯Ø§Ø±Ø¯</p>
        <p>â€¢ 60 Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¯Ø± Ù…Ø§Ù‡ Ø±Ø§ÛŒÚ¯Ø§Ù†</p>
        <p>â€¢ Ø¨Ù‡ØªØ±ÛŒÙ† Ú©ÛŒÙÛŒØª Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ</p>
        
        <h4>2ï¸âƒ£ Wit.ai:</h4>
        <p>â€¢ Ø¨Ù‡ <a href="https://wit.ai">Wit.ai</a> Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯</p>
        <p>â€¢ API Key Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:</p>
        <p><code>set WIT_AI_KEY=your_key_here</code></p>
        
        <h4>3ï¸âƒ£ Azure Speech:</h4>
        <p>â€¢ Ø¨Ù‡ <a href="https://portal.azure.com">Azure Portal</a> Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ Cognitive Services > Speech Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ API Key Ùˆ Region Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:</p>
        <p><code>set AZURE_SPEECH_KEY=your_key_here</code></p>
        <p><code>set AZURE_SPEECH_REGION=your_region_here</code></p>
        
        <h4>4ï¸âƒ£ Bing Speech:</h4>
        <p>â€¢ Ø¨Ù‡ <a href="https://azure.microsoft.com">Azure</a> Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ Bing Speech API ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ API Key Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:</p>
        <p><code>set BING_KEY=your_key_here</code></p>
        
        <h4>5ï¸âƒ£ Houndify:</h4>
        <p>â€¢ Ø¨Ù‡ <a href="https://www.houndify.com">Houndify</a> Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯</p>
        <p>â€¢ Client ID Ùˆ Client Key Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:</p>
        <p><code>set HOUNDIFY_CLIENT_ID=your_client_id</code></p>
        <p><code>set HOUNDIFY_CLIENT_KEY=your_client_key</code></p>
        
        <h4>6ï¸âƒ£ IBM Speech:</h4>
        <p>â€¢ Ø¨Ù‡ <a href="https://www.ibm.com/cloud/watson-speech-to-text">IBM Watson</a> Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯</p>
        <p>â€¢ Username Ùˆ Password Ø¯Ø±ÛŒØ§ÙØª Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:</p>
        <p><code>set IBM_USERNAME=your_username</code></p>
        <p><code>set IBM_PASSWORD=your_password</code></p>
        
        <h3>ğŸ’¡ Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</h3>
        <p>â€¢ Google Speech Ø¨Ù‡ØªØ±ÛŒÙ† Ú¯Ø²ÛŒÙ†Ù‡ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª</p>
        <p>â€¢ CMU Sphinx Ú©Ø§Ù…Ù„Ø§Ù‹ Ø¢ÙÙ„Ø§ÛŒÙ† Ø§Ø³Øª (ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)</p>
        <p>â€¢ Ø³Ø§ÛŒØ± Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ Ù†ÛŒØ§Ø² Ø¨Ù‡ API Key Ø¯Ø§Ø±Ù†Ø¯</p>
        <p>â€¢ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ø±Ø§ Ø¯Ø± Command Prompt ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯</p>
        
        <h3>ğŸ”— Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÛŒØ¯:</h3>
        <p>â€¢ <a href="https://wit.ai">Wit.ai</a></p>
        <p>â€¢ <a href="https://portal.azure.com">Azure Portal</a></p>
        <p>â€¢ <a href="https://www.houndify.com">Houndify</a></p>
        <p>â€¢ <a href="https://www.ibm.com/cloud/watson-speech-to-text">IBM Watson</a></p>
        """)
        
        layout.addWidget(text_edit)
        
        # Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
        button_layout = QHBoxLayout()
        
        open_wit_btn = QPushButton("Wit.ai")
        open_wit_btn.setStyleSheet("background-color: #4CAF50; color: white; padding: 8px;")
        open_wit_btn.clicked.connect(lambda: webbrowser.open("https://wit.ai"))
        
        open_azure_btn = QPushButton("Azure Portal")
        open_azure_btn.setStyleSheet("background-color: #2196F3; color: white; padding: 8px;")
        open_azure_btn.clicked.connect(lambda: webbrowser.open("https://portal.azure.com"))
        
        open_houndify_btn = QPushButton("Houndify")
        open_houndify_btn.setStyleSheet("background-color: #FF9800; color: white; padding: 8px;")
        open_houndify_btn.clicked.connect(lambda: webbrowser.open("https://www.houndify.com"))
        
        open_ibm_btn = QPushButton("IBM Watson")
        open_ibm_btn.setStyleSheet("background-color: #9C27B0; color: white; padding: 8px;")
        open_ibm_btn.clicked.connect(lambda: webbrowser.open("https://www.ibm.com/cloud/watson-speech-to-text"))
        
        close_btn = QPushButton("Ø¨Ø³ØªÙ†")
        close_btn.clicked.connect(dialog.accept)
        
        button_layout.addWidget(open_wit_btn)
        button_layout.addWidget(open_azure_btn)
        button_layout.addWidget(open_houndify_btn)
        button_layout.addWidget(open_ibm_btn)
        button_layout.addWidget(close_btn)
        
        layout.addLayout(button_layout)
        
        dialog.exec()
    
    def download_selected_model(self, dialog):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡"""
        # ØªØ´Ø®ÛŒØµ tab ÙØ¹Ø§Ù„
        current_tab = dialog.tab_widget.currentIndex()
        
        # ØªØ¹ÛŒÛŒÙ† model_list Ùˆ model_type Ø¨Ø± Ø§Ø³Ø§Ø³ tab
        if current_tab == 0:  # Vosk tab
            model_list = dialog.vosk_list
            model_type = "Vosk"
        elif current_tab == 1:  # Whisper tab
            model_list = dialog.whisper_list
            model_type = "Whisper"
        elif current_tab == 2:  # Hugging Face tab
            model_list = dialog.hf_list
            model_type = "HuggingFace"
        elif current_tab == 3:  # SpeechRecognition tab
            model_list = dialog.sr_list
            model_type = "SpeechRecognition"
        elif current_tab == 4:  # Silero tab
            model_list = dialog.silero_list
            model_type = "Silero"
        elif current_tab == 5:  # Kaldi tab
            model_list = dialog.kaldi_list
            model_type = "Kaldi"
        elif current_tab == 6:  # Iranian tab
            model_list = dialog.iranian_list
            model_type = "Iranian"
        else:
            QMessageBox.warning(dialog, "Ù‡Ø´Ø¯Ø§Ø±", "Ù„Ø·ÙØ§Ù‹ ÛŒÚ© tab Ù…Ø¹ØªØ¨Ø± Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")
            return
        
        current_row = model_list.currentRow()
        if current_row == -1:
            QMessageBox.warning(dialog, "Ù‡Ø´Ø¯Ø§Ø±", "Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")
            return
        
        # Ø¯Ø±ÛŒØ§ÙØª model_id
        model_ids = [key for key, value in ModelDownloader.DOWNLOADABLE_MODELS.items() 
                    if value["type"] == model_type]
        model_id = model_ids[current_row]
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù…Ø¯Ù„
        if model_type in ["SpeechRecognition", "Iranian"]:
            QMessageBox.information(dialog, "Ø§Ø·Ù„Ø§Ø¹Ø§Øª", f"Ù…Ø¯Ù„ {model_id} Ø¢Ù†Ù„Ø§ÛŒÙ† Ø§Ø³Øª Ùˆ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
            return
        
        if ModelDownloader.is_model_downloaded(model_id):
            QMessageBox.information(dialog, "Ø§Ø·Ù„Ø§Ø¹Ø§Øª", "Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
            return
        
        # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„
        QMessageBox.information(dialog, "Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯", f"Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_id} Ø´Ø±ÙˆØ¹ Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ØµØ¨Ø± Ú©Ù†ÛŒØ¯...")
        
        success, result = ModelDownloader.download_model(model_id)
        
        if success:
            QMessageBox.information(dialog, "Ù…ÙˆÙÙ‚", f"Ù…Ø¯Ù„ {model_id} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯!")
            dialog.accept()
        else:
            QMessageBox.critical(dialog, "Ø®Ø·Ø§", f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯: {result}")
    
    def download_all_vosk_models(self, dialog):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk"""
        reply = QMessageBox.question(
            dialog, "ØªØ£ÛŒÛŒØ¯", 
            "Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ØŸ\nØ§ÛŒÙ† Ú©Ø§Ø± Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ø¨Ø±Ø¯.",
            QMessageBox.Yes | QMessageBox.No,
            QMessageBox.No
        )
        
        if reply == QMessageBox.Yes:
            QMessageBox.information(dialog, "Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯", "Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk Ø´Ø±ÙˆØ¹ Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ØµØ¨Ø± Ú©Ù†ÛŒØ¯...")
            
            for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
                if model_info["type"] == "Vosk" and not ModelDownloader.is_model_downloaded(model_id):
                    success, result = ModelDownloader.download_model(model_id)
                    if not success:
                        QMessageBox.critical(dialog, "Ø®Ø·Ø§", f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_id}: {result}")
                        return
            
            QMessageBox.information(dialog, "Ù…ÙˆÙÙ‚", "Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯!")
            dialog.accept()
    
    def download_all_whisper_models(self, dialog):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper"""
        reply = QMessageBox.question(
            dialog, "ØªØ£ÛŒÛŒØ¯", 
            "Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ØŸ\nØ§ÛŒÙ† Ú©Ø§Ø± Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ø¨Ø±Ø¯.",
            QMessageBox.Yes | QMessageBox.No,
            QMessageBox.No
        )
        
        if reply == QMessageBox.Yes:
            QMessageBox.information(dialog, "Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯", "Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper Ø´Ø±ÙˆØ¹ Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ØµØ¨Ø± Ú©Ù†ÛŒØ¯...")
            
            for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
                if model_info["type"] == "Whisper" and not ModelDownloader.is_model_downloaded(model_id):
                    success, result = ModelDownloader.download_model(model_id)
                    if not success:
                        QMessageBox.critical(dialog, "Ø®Ø·Ø§", f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_id}: {result}")
                        return
            
            QMessageBox.information(dialog, "Ù…ÙˆÙÙ‚", "Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯!")
            dialog.accept()


if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = VoiceApp()
    window.show()
    sys.exit(app.exec())
