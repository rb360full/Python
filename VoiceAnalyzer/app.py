import sys
import os
from pathlib import Path
from PySide6.QtWidgets import (
    QApplication, QWidget, QPushButton, QVBoxLayout, QLabel, QFileDialog,
    QTextEdit, QScrollArea, QProgressBar, QMessageBox, QDialog, QComboBox,
    QDialogButtonBox, QFormLayout
)
from PySide6.QtCore import Qt, QThread, Signal
import subprocess
import json
import collections
import webbrowser
import tempfile
import time
import requests
from tqdm import tqdm
import zipfile
import shutil

# Whisper Ùˆ ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ†
import whisper

# ØªØµØ­ÛŒØ­ Ø®ÙˆØ¯Ú©Ø§Ø± ÙØ§Ø±Ø³ÛŒ
try:
    from persiantools import digits
    from persiantools import characters
    PERSIAN_TOOLS_AVAILABLE = True
except ImportError:
    PERSIAN_TOOLS_AVAILABLE = False
    print("Warning: persiantools not installed. Install with: pip install persiantools")

# Google Speech-to-Text
try:
    from google.cloud import speech
    GOOGLE_SPEECH_AVAILABLE = True
except ImportError:
    GOOGLE_SPEECH_AVAILABLE = False
    print("Warning: google-cloud-speech not installed. Install with: pip install google-cloud-speech")

# Vosk Speech Recognition
try:
    import vosk
    import json
    VOSK_AVAILABLE = True
except ImportError:
    VOSK_AVAILABLE = False
    print("Warning: vosk not installed. Install with: pip install vosk")

# Microsoft Azure Speech
try:
    import azure.cognitiveservices.speech as speechsdk
    AZURE_SPEECH_AVAILABLE = True
except ImportError:
    AZURE_SPEECH_AVAILABLE = False
    print("Warning: azure-cognitiveservices-speech not installed. Install with: pip install azure-cognitiveservices-speech")

# AssemblyAI
try:
    import assemblyai as aai
    ASSEMBLYAI_AVAILABLE = True
except ImportError:
    ASSEMBLYAI_AVAILABLE = False
    print("Warning: assemblyai not installed. Install with: pip install assemblyai")

# Hugging Face Transformers
try:
    from transformers import pipeline, AutoModelForCTC, AutoProcessor
    import torch
    HUGGINGFACE_AVAILABLE = True
except ImportError:
    HUGGINGFACE_AVAILABLE = False
    print("Warning: transformers not installed. Install with: pip install transformers torch")

# SpeechRecognition
try:
    import speech_recognition as sr
    SPEECHRECOGNITION_AVAILABLE = True
except ImportError:
    SPEECHRECOGNITION_AVAILABLE = False
    print("Warning: SpeechRecognition not installed. Install with: pip install SpeechRecognition")

# Silero STT
try:
    import torch
    import torchaudio
    SILERO_AVAILABLE = True
except ImportError:
    SILERO_AVAILABLE = False
    print("Warning: torchaudio not installed. Install with: pip install torchaudio")

# Kaldi
try:
    import kaldi_io
    KALDI_AVAILABLE = True
except ImportError:
    KALDI_AVAILABLE = False
    print("Warning: kaldi-io not installed. Install with: pip install kaldi-io")

# Ø¨Ø±Ø±Ø³ÛŒ ffmpeg Ø¯Ø± PATH
try:
    subprocess.run(["ffmpeg", "-version"], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
except FileNotFoundError:
    raise RuntimeError("FFmpeg not found in PATH! Install it or add to PATH.")

# Ù…Ø³ÛŒØ± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ú©Ø§Ø³ØªÙˆÙ…
CUSTOM_DICT_FILE = Path("custom_dict.json")
if not CUSTOM_DICT_FILE.exists():
    with open(CUSTOM_DICT_FILE, "w", encoding="utf-8") as f:
        json.dump({}, f, ensure_ascii=False, indent=2)

# Ù…Ø³ÛŒØ± ØªÚ©ÛŒÙ‡ Ú©Ù„Ø§Ù…â€ŒÙ‡Ø§
RELYING_DICT_FILE = Path("relying_dict.json")
if not RELYING_DICT_FILE.exists():
    with open(RELYING_DICT_FILE, "w", encoding="utf-8") as f:
        json.dump({"ØªÚ©ÛŒÙ‡_Ú©Ù„Ø§Ù…": ["Ø®Ø¨", "Ø¢Ø®Ù‡", "ÛŒØ¹Ù†ÛŒ", "Ø­Ø§Ù„Ø§"]}, f, ensure_ascii=False, indent=2)

# Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ØªÙ†Ø¸ÛŒÙ…Ø§Øª
CONFIG_FILE = Path("voice_analyzer_config.json")
if not CONFIG_FILE.exists():
    with open(CONFIG_FILE, "w", encoding="utf-8") as f:
        json.dump({
            "selected_model": "vosk_persian",
            "last_audio_path": "",
            "window_geometry": None,
            "preferred_language": "fa"
        }, f, ensure_ascii=False, indent=2)

# Import Ù¾Ù†Ø¬Ø±Ù‡ Ù…Ø¯ÛŒØ±ÛŒØª Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ
from custom_dict_manager import DictManager

class ConfigManager:
    """Ù…Ø¯ÛŒØ±ÛŒØª ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ù†Ø§Ù…Ù‡"""
    
    @staticmethod
    def load_config():
        """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„"""
        try:
            with open(CONFIG_FILE, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª: {e}")
            return {
                "selected_model": "vosk_persian",
                "last_audio_path": "",
                "window_geometry": None,
                "preferred_language": "fa"
            }
    
    @staticmethod
    def save_config(config):
        """Ø°Ø®ÛŒØ±Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¯Ø± ÙØ§ÛŒÙ„"""
        try:
            with open(CONFIG_FILE, "w", encoding="utf-8") as f:
                json.dump(config, f, ensure_ascii=False, indent=2)
            return True
        except Exception as e:
            print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª: {e}")
            return False
    
    @staticmethod
    def update_model(model_name):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡"""
        config = ConfigManager.load_config()
        config["selected_model"] = model_name
        return ConfigManager.save_config(config)
    
    @staticmethod
    def update_audio_path(audio_path):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…Ø³ÛŒØ± Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ"""
        config = ConfigManager.load_config()
        config["last_audio_path"] = audio_path
        return ConfigManager.save_config(config)
    
    @staticmethod
    def update_window_geometry(geometry):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØª Ùˆ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡"""
        config = ConfigManager.load_config()
        config["window_geometry"] = geometry
        return ConfigManager.save_config(config)

class ModelDownloader:
    """Ú©Ù„Ø§Ø³ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø®ÙˆØ¯Ú©Ø§Ø± Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
    
    # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯
    DOWNLOADABLE_MODELS = {
        # Vosk Models (ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)
        "vosk_persian": {
            "url": "https://alphacephei.com/vosk/models/vosk-model-fa-0.5.zip",
            "name": "vosk-model-fa-0.5", 
            "size": "1.13 GB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "âœ… Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ",
            "type": "Vosk"
        },
        "vosk_small": {
            "url": "https://alphacephei.com/vosk/models/vosk-model-small-en-us-0.15.zip",
            "name": "vosk-model-small-en-us-0.15",
            "size": "40 MB",
            "language": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "warning": "âš ï¸ ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "type": "Vosk"
        },
        "vosk_large": {
            "url": "https://alphacephei.com/vosk/models/vosk-model-en-us-0.22.zip", 
            "name": "vosk-model-en-us-0.22",
            "size": "1.8 GB",
            "language": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "warning": "âš ï¸ ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "type": "Vosk"
        },
        
        # Whisper Models
        "whisper_tiny": {
            "url": "whisper://tiny",
            "name": "whisper-tiny",
            "size": "75 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âš ï¸ Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ",
            "type": "Whisper"
        },
        "whisper_base": {
            "url": "whisper://base",
            "name": "whisper-base",
            "size": "142 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âš ï¸ Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ",
            "type": "Whisper"
        },
        "whisper_small": {
            "url": "whisper://small",
            "name": "whisper-small",
            "size": "466 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… ØªØ¹Ø§Ø¯Ù„ Ø®ÙˆØ¨",
            "type": "Whisper"
        },
        "whisper_medium": {
            "url": "whisper://medium",
            "name": "whisper-medium",
            "size": "1.5 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§",
            "type": "Whisper"
        },
        "whisper_large": {
            "url": "whisper://large",
            "name": "whisper-large",
            "size": "2.9 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¯Ù‚Øª",
            "type": "Whisper"
        },
        "whisper_large_v2": {
            "url": "whisper://large-v2",
            "name": "whisper-large-v2",
            "size": "2.9 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡",
            "type": "Whisper"
        },
        "whisper_large_v3": {
            "url": "whisper://large-v3",
            "name": "whisper-large-v3",
            "size": "2.9 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡",
            "type": "Whisper"
        },
        
        # Hugging Face Transformers
        "hf_wav2vec2_persian": {
            "url": "huggingface://m3hrdadfi/wav2vec2-large-xlsr-53-persian",
            "name": "wav2vec2-large-xlsr-53-persian",
            "size": "1.2 GB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "âœ… Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ - Hugging Face",
            "type": "HuggingFace"
        },
        "hf_whisper_persian": {
            "url": "huggingface://m3hrdadfi/whisper-persian",
            "name": "whisper-persian",
            "size": "1.5 GB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "âœ… Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ - Hugging Face",
            "type": "HuggingFace"
        },
        "hf_whisper_tiny": {
            "url": "huggingface://openai/whisper-tiny",
            "name": "whisper-tiny-hf",
            "size": "75 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âš ï¸ Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ",
            "type": "HuggingFace"
        },
        "hf_whisper_base": {
            "url": "huggingface://openai/whisper-base",
            "name": "whisper-base-hf",
            "size": "142 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âš ï¸ Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ",
            "type": "HuggingFace"
        },
        "hf_whisper_small": {
            "url": "huggingface://openai/whisper-small",
            "name": "whisper-small-hf",
            "size": "466 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… ØªØ¹Ø§Ø¯Ù„ Ø®ÙˆØ¨",
            "type": "HuggingFace"
        },
        "hf_whisper_medium": {
            "url": "huggingface://openai/whisper-medium",
            "name": "whisper-medium-hf",
            "size": "1.5 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§",
            "type": "HuggingFace"
        },
        "hf_whisper_large": {
            "url": "huggingface://openai/whisper-large-v2",
            "name": "whisper-large-v2-hf",
            "size": "2.9 GB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¯Ù‚Øª",
            "type": "HuggingFace"
        },
        
        # SpeechRecognition
        "speechrecognition_google": {
            "url": "speechrecognition://google",
            "name": "Google Speech Recognition",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† 60Ø¯Ù‚ÛŒÙ‚Ù‡/Ù…Ø§Ù‡",
            "type": "SpeechRecognition"
        },
        "speechrecognition_sphinx": {
            "url": "speechrecognition://sphinx",
            "name": "CMU Sphinx",
            "size": "100 MB",
            "language": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "warning": "âš ï¸ ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "type": "SpeechRecognition"
        },
        "speechrecognition_wit": {
            "url": "speechrecognition://wit",
            "name": "Wit.ai",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ",
            "type": "SpeechRecognition"
        },
        "speechrecognition_azure": {
            "url": "speechrecognition://azure",
            "name": "Azure Speech",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† 5Ø³Ø§Ø¹Øª/Ù…Ø§Ù‡",
            "type": "SpeechRecognition"
        },
        "speechrecognition_bing": {
            "url": "speechrecognition://bing",
            "name": "Bing Speech",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ",
            "type": "SpeechRecognition"
        },
        "speechrecognition_houndify": {
            "url": "speechrecognition://houndify",
            "name": "Houndify",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ",
            "type": "SpeechRecognition"
        },
        "speechrecognition_ibm": {
            "url": "speechrecognition://ibm",
            "name": "IBM Speech to Text",
            "size": "0 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "ğŸŒ Ø¢Ù†Ù„Ø§ÛŒÙ† - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ",
            "type": "SpeechRecognition"
        },
        
        # Silero STT
        "silero_stt_en": {
            "url": "silero://stt_en",
            "name": "Silero STT English",
            "size": "50 MB",
            "language": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "warning": "âš ï¸ ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "type": "Silero"
        },
        "silero_stt_multilingual": {
            "url": "silero://stt_multilingual",
            "name": "Silero STT Multilingual",
            "size": "200 MB",
            "language": "Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡",
            "warning": "âœ… Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ÙØ§Ø±Ø³ÛŒ",
            "type": "Silero"
        },
        
        # Kaldi
        "kaldi_persian": {
            "url": "kaldi://persian",
            "name": "Kaldi Persian Model",
            "size": "500 MB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "âœ… Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ",
            "type": "Kaldi"
        },
        "kaldi_english": {
            "url": "kaldi://english",
            "name": "Kaldi English Model",
            "size": "300 MB",
            "language": "Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "warning": "âš ï¸ ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ",
            "type": "Kaldi"
        },
        
        # Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¨ÙˆÙ…ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
        "iranian_arvan": {
            "url": "arvan://speech",
            "name": "Arvan Cloud Speech",
            "size": "0 MB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "ğŸ‡®ğŸ‡· Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ - Ø¢Ù†Ù„Ø§ÛŒÙ†",
            "type": "Iranian"
        },
        "iranian_fanap": {
            "url": "fanap://speech",
            "name": "Fanap Speech API",
            "size": "0 MB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "ğŸ‡®ğŸ‡· Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ - Ø¢Ù†Ù„Ø§ÛŒÙ†",
            "type": "Iranian"
        },
        "iranian_parsijoo": {
            "url": "parsijoo://speech",
            "name": "Parsijoo Speech",
            "size": "0 MB",
            "language": "ÙØ§Ø±Ø³ÛŒ",
            "warning": "ğŸ‡®ğŸ‡· Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ - Ø¢Ù†Ù„Ø§ÛŒÙ†",
            "type": "Iranian"
        }
    }
    
    # Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ Ø¨Ø§ Ú©Ø¯ Ù‚Ø¨Ù„ÛŒ
    VOSK_MODELS = {
        key: value for key, value in DOWNLOADABLE_MODELS.items() 
        if value["type"] == "Vosk"
    }
    
    @staticmethod
    def download_model(model_id, progress_callback=None, progress_bar_callback=None):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„"""
        if model_id not in ModelDownloader.DOWNLOADABLE_MODELS:
            return False, f"Ù…Ø¯Ù„ {model_id} Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯"
        
        model_info = ModelDownloader.DOWNLOADABLE_MODELS[model_id]
        
        # Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper
        if model_info["type"] == "Whisper":
            return ModelDownloader._download_whisper_model(model_id, model_info, progress_callback, progress_bar_callback)
        
        # Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk
        elif model_info["type"] == "Vosk":
            return ModelDownloader._download_vosk_model(model_id, model_info, progress_callback, progress_bar_callback)
        
        return False, f"Ù†ÙˆØ¹ Ù…Ø¯Ù„ {model_info['type']} Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯"
    
    @staticmethod
    def _download_whisper_model(model_id, model_info, progress_callback=None, progress_bar_callback=None):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Whisper"""
        try:
            if progress_callback:
                progress_callback(f"Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_info['name']} ({model_info['size']})...")
            
            # Whisper Ø®ÙˆØ¯Ø´ Ù…Ø¯Ù„â€ŒÙ‡Ø§ Ø±Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…ÛŒâ€ŒÚ©Ù†Ù‡
            model_name = model_id.replace("whisper_", "")
            # ØªØ¨Ø¯ÛŒÙ„ Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Whisper
            if model_name == "large_v2":
                model_name = "large-v2"
            elif model_name == "large_v3":
                model_name = "large-v3"
            
            model = whisper.load_model(model_name)
            
            if progress_bar_callback:
                progress_bar_callback(100)
            
            return True, f"Ù…Ø¯Ù„ {model_name} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯"
            
        except Exception as e:
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ Whisper: {str(e)}"
    
    @staticmethod
    def _download_vosk_model(model_id, model_info, progress_callback=None, progress_bar_callback=None):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Vosk"""
        models_dir = Path.home() / ".vosk" / "models"
        models_dir.mkdir(parents=True, exist_ok=True)
        
        model_path = models_dir / model_info["name"]
        
        # Ø§Ú¯Ø± Ù…Ø¯Ù„ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡
        if model_path.exists():
            return True, str(model_path)
        
        try:
            # Ø¯Ø§Ù†Ù„ÙˆØ¯ ÙØ§ÛŒÙ„
            if progress_callback:
                progress_callback(f"Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_info['name']} ({model_info['size']})...")
            
            response = requests.get(model_info["url"], stream=True)
            response.raise_for_status()
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ ÙØ§ÛŒÙ„
            total_size = int(response.headers.get('content-length', 0))
            downloaded_size = 0
            
            # Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„
            zip_path = models_dir / f"{model_info['name']}.zip"
            
            with open(zip_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    if chunk:
                        f.write(chunk)
                        downloaded_size += len(chunk)
                        
                        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ progress bar
                        if progress_bar_callback and total_size > 0:
                            progress_percent = int((downloaded_size / total_size) * 100)
                            progress_bar_callback(progress_percent)
            
            # Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙØ§ÛŒÙ„
            if progress_callback:
                progress_callback("Ø¯Ø± Ø­Ø§Ù„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ø¯Ù„...")
            
            if progress_bar_callback:
                progress_bar_callback(100)  # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ø§Ù…Ù„
            
            with zipfile.ZipFile(zip_path, 'r') as zip_ref:
                zip_ref.extractall(models_dir)
            
            # Ø­Ø°Ù ÙØ§ÛŒÙ„ zip
            zip_path.unlink()
            
            return True, str(model_path)
            
        except Exception as e:
            return False, f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯: {str(e)}"
    
    @staticmethod
    def is_model_downloaded(model_id):
        """Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„"""
        if model_id not in ModelDownloader.DOWNLOADABLE_MODELS:
            return False
        
        model_info = ModelDownloader.DOWNLOADABLE_MODELS[model_id]
        
        # Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper
        if model_info["type"] == "Whisper":
            try:
                model_name = model_id.replace("whisper_", "")
                # ØªØ¨Ø¯ÛŒÙ„ Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Whisper
                if model_name == "large_v2":
                    model_name = "large-v2"
                elif model_name == "large_v3":
                    model_name = "large-v3"
                
                # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ù…Ø¯Ù„ Ø¯Ø± cache Whisper
                import whisper
                model_path = whisper._MODELS.get(model_name)
                return model_path is not None
            except:
                return False
        
        # Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk
        elif model_info["type"] == "Vosk":
            models_dir = Path.home() / ".vosk" / "models"
            model_path = models_dir / model_info["name"]
            return model_path.exists()
        
        return False

class ModelSelectionDialog(QDialog):
    def __init__(self, parent=None):
        super().__init__(parent)
        self.setWindowTitle("Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Speech-to-Text")
        self.setModal(True)
        self.resize(500, 300)
        
        layout = QFormLayout(self)
        
        # Ù„ÛŒØ³Øª Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Speech-to-Text (ÙÙ‚Ø· ÙØ§Ø±Ø³ÛŒ Ùˆ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ)
        self.model_combo = QComboBox()
        models = [
            # Vosk Models (Ú©Ø§Ù…Ù„Ø§Ù‹ Ø±Ø§ÛŒÚ¯Ø§Ù†)
            ("vosk_persian", "âœ… Vosk Persian - Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ (1.13 GB)"),
            ("vosk_small", "âš ï¸ Vosk Small - ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (40 MB)"),
            ("vosk_large", "âš ï¸ Vosk Large - ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (1.8 GB)"),
            
            # Whisper Models (Ú†Ù†Ø¯ Ø²Ø¨Ø§Ù†Ù‡)
            ("whisper_tiny", "âš ï¸ Whisper Tiny - Ø®ÛŒÙ„ÛŒ Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ (75 MB)"),
            ("whisper_base", "âš ï¸ Whisper Base - Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ (142 MB)"),
            ("whisper_small", "âœ… Whisper Small - ØªØ¹Ø§Ø¯Ù„ Ø®ÙˆØ¨ (466 MB)"),
            ("whisper_medium", "âœ… Whisper Medium - Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ (1.5 GB)"),
            ("whisper_large", "âœ… Whisper Large - Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¯Ù‚Øª (2.9 GB)"),
            ("whisper_large_v2", "âœ… Whisper Large V2 - Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡ (2.9 GB)"),
            ("whisper_large_v3", "âœ… Whisper Large V3 - Ø¬Ø¯ÛŒØ¯ØªØ±ÛŒÙ† Ù†Ø³Ø®Ù‡ (2.9 GB)"),
            
            # Hugging Face Transformers
            ("hf_wav2vec2_persian", "âœ… Wav2Vec2 Persian - Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ (1.2 GB)"),
            ("hf_whisper_persian", "âœ… Whisper Persian - Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ (1.5 GB)"),
            ("hf_whisper_tiny", "âš ï¸ Whisper Tiny HF - Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ (75 MB)"),
            ("hf_whisper_base", "âš ï¸ Whisper Base HF - Ø¶Ø¹ÛŒÙ Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ (142 MB)"),
            ("hf_whisper_small", "âœ… Whisper Small HF - ØªØ¹Ø§Ø¯Ù„ Ø®ÙˆØ¨ (466 MB)"),
            ("hf_whisper_medium", "âœ… Whisper Medium HF - Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ (1.5 GB)"),
            ("hf_whisper_large", "âœ… Whisper Large HF - Ø¨Ø§Ù„Ø§ØªØ±ÛŒÙ† Ø¯Ù‚Øª (2.9 GB)"),
            
            # SpeechRecognition
            ("speechrecognition_google", "ğŸŒ Google Speech - Ø±Ø§ÛŒÚ¯Ø§Ù† 60Ø¯Ù‚ÛŒÙ‚Ù‡/Ù…Ø§Ù‡ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("speechrecognition_sphinx", "âš ï¸ CMU Sphinx - ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (100 MB)"),
            ("speechrecognition_wit", "ğŸŒ Wit.ai - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("speechrecognition_azure", "ğŸŒ Azure Speech - Ø±Ø§ÛŒÚ¯Ø§Ù† 5Ø³Ø§Ø¹Øª/Ù…Ø§Ù‡ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("speechrecognition_bing", "ğŸŒ Bing Speech - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("speechrecognition_houndify", "ğŸŒ Houndify - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("speechrecognition_ibm", "ğŸŒ IBM Speech - Ø±Ø§ÛŒÚ¯Ø§Ù† ØªØ§ Ø­Ø¯ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            
            # Silero STT
            ("silero_stt_en", "âš ï¸ Silero STT English - ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (50 MB)"),
            ("silero_stt_multilingual", "âœ… Silero STT Multilingual - Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² ÙØ§Ø±Ø³ÛŒ (200 MB)"),
            
            # Kaldi
            ("kaldi_persian", "âœ… Kaldi Persian - Ù…Ø®ØµÙˆØµ ÙØ§Ø±Ø³ÛŒ (500 MB)"),
            ("kaldi_english", "âš ï¸ Kaldi English - ÙÙ‚Ø· Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ (300 MB)"),
            
            # Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¨ÙˆÙ…ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
            ("iranian_arvan", "ğŸ‡®ğŸ‡· Arvan Cloud Speech - Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("iranian_fanap", "ğŸ‡®ğŸ‡· Fanap Speech API - Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("iranian_parsijoo", "ğŸ‡®ğŸ‡· Parsijoo Speech - Ø³Ø±ÙˆÛŒØ³ Ø§ÛŒØ±Ø§Ù†ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            
            # Google Models (Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø²Ú¯Ø§Ø±ÛŒ)
            ("google_standard", "ğŸŒ Google Standard - Ø±Ø§ÛŒÚ¯Ø§Ù† 60Ø¯Ù‚ÛŒÙ‚Ù‡/Ù…Ø§Ù‡ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("google_enhanced", "ğŸ’³ Google Enhanced - Ù¾ÙˆÙ„ÛŒØŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ± (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("google_phone_call", "ğŸ’³ Google Phone Call - Ù¾ÙˆÙ„ÛŒØŒ Ù…Ø®ØµÙˆØµ ØªÙ…Ø§Ø³â€ŒÙ‡Ø§ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("google_medical", "ğŸ’³ Google Medical - Ù¾ÙˆÙ„ÛŒØŒ Ø§ØµØ·Ù„Ø§Ø­Ø§Øª Ù¾Ø²Ø´Ú©ÛŒ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("google_video", "ğŸ’³ Google Video - Ù¾ÙˆÙ„ÛŒØŒ Ù…Ø®ØµÙˆØµ ÙˆÛŒØ¯ÛŒÙˆÙ‡Ø§ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            
            # Microsoft Azure Speech
            ("azure_standard", "ğŸŒ Azure Standard - Ø±Ø§ÛŒÚ¯Ø§Ù† 5Ø³Ø§Ø¹Øª/Ù…Ø§Ù‡ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("azure_enhanced", "ğŸ’³ Azure Enhanced - Ù¾ÙˆÙ„ÛŒØŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ± (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            
            # AssemblyAI
            ("assemblyai_standard", "ğŸŒ AssemblyAI - Ø±Ø§ÛŒÚ¯Ø§Ù† 3Ø³Ø§Ø¹Øª/Ù…Ø§Ù‡ (Ø¢Ù†Ù„Ø§ÛŒÙ†)"),
            ("assemblyai_enhanced", "ğŸ’³ AssemblyAI Enhanced - Ù¾ÙˆÙ„ÛŒØŒ Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ± (Ø¢Ù†Ù„Ø§ÛŒÙ†)")
        ]
        
        for model_id, description in models:
            self.model_combo.addItem(f"{model_id} - {description}", model_id)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¢Ø®Ø±ÛŒÙ† Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
        config = ConfigManager.load_config()
        last_model = config.get("selected_model", "vosk_persian")
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…Ø¯Ù„ Ø¢Ø®Ø±
        model_index = 0
        for i, (model_id, _) in enumerate(models):
            if model_id == last_model:
                model_index = i
                break
        
        self.model_combo.setCurrentIndex(model_index)
        
        layout.addRow("Ù…Ø¯Ù„:", self.model_combo)
        
        # Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
        buttons = QDialogButtonBox(QDialogButtonBox.Ok | QDialogButtonBox.Cancel)
        buttons.accepted.connect(self.accept)
        buttons.rejected.connect(self.reject)
        layout.addRow(buttons)
    
    def get_selected_model(self):
        return self.model_combo.currentData()

def improve_persian_text(text):
    """Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ Ø¨Ø§ ØªØµØ­ÛŒØ­ Ø®ÙˆØ¯Ú©Ø§Ø±"""
    if not PERSIAN_TOOLS_AVAILABLE:
        return text
    
    # Ø­Ø°Ù ØªÚ©Ø±Ø§Ø±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ Ú©Ù„Ù…Ø§Øª
    words = text.split()
    cleaned_words = []
    prev_word = None
    repeat_count = 0
    
    for word in words:
        if word == prev_word:
            repeat_count += 1
            if repeat_count < 3:  # Ø­Ø¯Ø§Ú©Ø«Ø± 3 ØªÚ©Ø±Ø§Ø± Ù…Ø¬Ø§Ø²
                cleaned_words.append(word)
        else:
            repeat_count = 0
            cleaned_words.append(word)
        prev_word = word
    
    text = " ".join(cleaned_words)
    
    # ØªØµØ­ÛŒØ­ Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
    text = characters.ar_to_fa(text)  # ØªØ¨Ø¯ÛŒÙ„ Ø¹Ø±Ø¨ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
    
    # ØªØµØ­ÛŒØ­ Ø§Ø¹Ø¯Ø§Ø¯
    text = digits.en_to_fa(text)  # ØªØ¨Ø¯ÛŒÙ„ Ø§Ø¹Ø¯Ø§Ø¯ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ
    
    return text

class TranscribeThread(QThread):
    progress = Signal(int)
    finished = Signal(str)
    download_progress = Signal(int)
    download_status = Signal(str)

    def __init__(self, audio_path, model_name="vosk_persian"):
        super().__init__()
        self.audio_path = audio_path
        self.model_name = model_name
        self.model = None
        self._pause = False
        self._stop = False

    def run(self):
        try:
            self.progress.emit(5)
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
            if self.model_name.startswith("whisper_"):
                whisper_model = self.model_name.replace("whisper_", "")
                # ØªØ¨Ø¯ÛŒÙ„ Ù†Ø§Ù… Ù…Ø¯Ù„ Ø¨Ø±Ø§ÛŒ Whisper
                if whisper_model == "large_v2":
                    whisper_model = "large-v2"
                elif whisper_model == "large_v3":
                    whisper_model = "large-v3"
                self.model = whisper.load_model(whisper_model)
            elif self.model_name.startswith("google_"):
                if not GOOGLE_SPEECH_AVAILABLE:
                    self.finished.emit("Error: Google Speech-to-Text not installed. Install with: pip install google-cloud-speech")
                    return
                self.model = "google"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Google
            elif self.model_name.startswith("vosk_"):
                if not VOSK_AVAILABLE:
                    self.finished.emit("Error: Vosk not installed. Install with: pip install vosk")
                    return
                self.model = "vosk"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Vosk
            elif self.model_name.startswith("azure_"):
                if not AZURE_SPEECH_AVAILABLE:
                    self.finished.emit("Error: Azure Speech not installed. Install with: pip install azure-cognitiveservices-speech")
                    return
                self.model = "azure"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Azure
            elif self.model_name.startswith("assemblyai_"):
                if not ASSEMBLYAI_AVAILABLE:
                    self.finished.emit("Error: AssemblyAI not installed. Install with: pip install assemblyai")
                    return
                self.model = "assemblyai"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AssemblyAI
            elif self.model_name.startswith("hf_"):
                if not HUGGINGFACE_AVAILABLE:
                    self.finished.emit("Error: Hugging Face Transformers not installed. Install with: pip install transformers torch")
                    return
                self.model = "huggingface"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Hugging Face
            elif self.model_name.startswith("speechrecognition_"):
                if not SPEECHRECOGNITION_AVAILABLE:
                    self.finished.emit("Error: SpeechRecognition not installed. Install with: pip install SpeechRecognition")
                    return
                self.model = "speechrecognition"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² SpeechRecognition
            elif self.model_name.startswith("silero_"):
                if not SILERO_AVAILABLE:
                    self.finished.emit("Error: Silero STT not installed. Install with: pip install torchaudio")
                    return
                self.model = "silero"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Silero
            elif self.model_name.startswith("kaldi_"):
                if not KALDI_AVAILABLE:
                    self.finished.emit("Error: Kaldi not installed. Install with: pip install kaldi-io")
                    return
                self.model = "kaldi"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Kaldi
            elif self.model_name.startswith("iranian_"):
                self.model = "iranian"  # Ù†Ø´Ø§Ù†Ú¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
            else:
                self.finished.emit(f"Error: Unknown model {self.model_name}")
                return
                
            self.progress.emit(10)
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ WAV 16kHz Mono Ø¯Ø± ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª
            with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmp:
                temp_wav = tmp.name

            subprocess.run([
                "ffmpeg", "-y", "-i", str(self.audio_path),
                "-ar", "16000", "-ac", "1", 
                "-af", "highpass=f=80,lowpass=f=8000",  # ÙÛŒÙ„ØªØ± ØµØ¯Ø§ Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ú©ÛŒÙÛŒØª
                temp_wav
            ], check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)
            self.progress.emit(35)

            # ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ†
            if self.model == "google":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Google Speech-to-Text
                text = self.transcribe_with_google(temp_wav)
            elif self.model == "vosk":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Vosk
                text = self.transcribe_with_vosk(temp_wav)
            elif self.model == "azure":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Azure Speech
                text = self.transcribe_with_azure(temp_wav)
            elif self.model == "assemblyai":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AssemblyAI
                text = self.transcribe_with_assemblyai(temp_wav)
            elif self.model == "huggingface":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Hugging Face
                text = self.transcribe_with_huggingface(temp_wav)
            elif self.model == "speechrecognition":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² SpeechRecognition
                text = self.transcribe_with_speechrecognition(temp_wav)
            elif self.model == "silero":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Silero STT
                text = self.transcribe_with_silero(temp_wav)
            elif self.model == "kaldi":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Kaldi
                text = self.transcribe_with_kaldi(temp_wav)
            elif self.model == "iranian":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ
                text = self.transcribe_with_iranian(temp_wav)
            else:
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Whisper
                result = self.model.transcribe(
                    temp_wav,
                    language="fa",  # Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ
                    initial_prompt="Ø§ÛŒÙ† ÛŒÚ© ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ù…ØªÙ† Ø±Ø§ Ø¨Ø§ Ø§Ù…Ù„Ø§ÛŒ ØµØ­ÛŒØ­ ÙØ§Ø±Ø³ÛŒ Ø¨Ù†ÙˆÛŒØ³ÛŒØ¯.",
                    temperature=0.0,  # Ú©Ø§Ù‡Ø´ Ø®Ù„Ø§Ù‚ÛŒØª Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±
                    beam_size=5,  # Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ± Ø¨Ø±Ø§ÛŒ Ú©Ù„Ù…Ø§Øª
                    best_of=3,  # ØªØ³Øª Ú†Ù†Ø¯ÛŒÙ† Ø¨Ø§Ø± Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø¨Ù‡ØªØ±ÛŒÙ† Ù†ØªÛŒØ¬Ù‡
                    patience=1.0,  # ØµØ¨Ø± Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±
                    length_penalty=1.0,  # ØªÙ†Ø¸ÛŒÙ… Ø·ÙˆÙ„ Ù…ØªÙ†
                    suppress_tokens=[-1],  # Ø­Ø°Ù ØªÙˆÚ©Ù†â€ŒÙ‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
                    word_timestamps=True,  # ØªØ´Ø®ÛŒØµ Ø¨Ù‡ØªØ± Ú©Ù„Ù…Ø§Øª
                    no_speech_threshold=0.6,  # Ø¢Ø³ØªØ§Ù†Ù‡ ØªØ´Ø®ÛŒØµ Ø³Ú©ÙˆØª
                    logprob_threshold=-1.0,  # Ø¢Ø³ØªØ§Ù†Ù‡ Ø§Ø­ØªÙ…Ø§Ù„ Ú©Ù„Ù…Ø§Øª
                    compression_ratio_threshold=2.4  # Ø¢Ø³ØªØ§Ù†Ù‡ ÙØ´Ø±Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ
                )
                text = result["text"]
            
            # Ø¨Ù‡Ø¨ÙˆØ¯ Ù…ØªÙ† ÙØ§Ø±Ø³ÛŒ
            text = improve_persian_text(text)
            
            self.progress.emit(75)

            # Ù„ØºØ§Øª Ú©Ø§Ø³ØªÙˆÙ…
            with open(CUSTOM_DICT_FILE, encoding="utf-8") as f:
                custom_dict = json.load(f)
            for wrong, correct in custom_dict.items():
                text = text.replace(wrong, correct)

            # Ø¢Ù…Ø§Ø±
            words = text.split()
            word_count = len(words)

            # ØªÚ©ÛŒÙ‡ Ú©Ù„Ø§Ù…â€ŒÙ‡Ø§
            with open(RELYING_DICT_FILE, encoding="utf-8") as f:
                relying_data = json.load(f)
            relying_words = relying_data.get("ØªÚ©ÛŒÙ‡_Ú©Ù„Ø§Ù…", [])
            detected_relying = [w for w in words if w in relying_words]

            counter = collections.Counter(words)
            most_common = counter.most_common(10)

            stats = "\n\n--- Statistics ---\n"
            stats += f"Word Count: {word_count}\n"
            stats += f"Detected Pauses/Tic Words: {', '.join(detected_relying)}\n"
            stats += "Top 10 Frequent Words:\n"
            for w, c in most_common:
                stats += f"{w}: {c}\n"

            text += stats
            self.progress.emit(100)
            self.finished.emit(text)

            # Ø­Ø°Ù ÙØ§ÛŒÙ„ Ù…ÙˆÙ‚Øª
            os.remove(temp_wav)

        except Exception as e:
            self.finished.emit(f"Error: {str(e)}")
    
    def transcribe_with_google(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Google Speech-to-Text"""
        try:
            client = speech.SpeechClient()
            
            # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            with open(audio_file, "rb") as audio_file_content:
                content = audio_file_content.read()
            
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª ØªØ´Ø®ÛŒØµ
            audio = speech.RecognitionAudio(content=content)
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„ Google
            if self.model_name == "google_enhanced":
                model = "phone_call"  # Ù…Ø¯Ù„ Enhanced
            elif self.model_name == "google_phone_call":
                model = "phone_call"
            elif self.model_name == "google_medical":
                model = "medical_dictation"
            elif self.model_name == "google_video":
                model = "video"
            else:
                model = "default"  # Ù…Ø¯Ù„ Standard
            
            config = speech.RecognitionConfig(
                encoding=speech.RecognitionConfig.AudioEncoding.LINEAR16,
                sample_rate_hertz=16000,
                language_code="fa-IR",  # ÙØ§Ø±Ø³ÛŒ Ø§ÛŒØ±Ø§Ù†
                model=model,
                enable_automatic_punctuation=True,
                enable_word_time_offsets=True
            )
            
            # ØªØ´Ø®ÛŒØµ
            response = client.recognize(config=config, audio=audio)
            
            # ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬
            text = ""
            for result in response.results:
                text += result.alternatives[0].transcript + " "
            
            return text.strip()
            
        except Exception as e:
            error_msg = str(e)
            if "credentials" in error_msg.lower():
                return f"""Google Speech Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… credentials

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Google Speech-to-Text:

1. Ø¨Ù‡ https://console.cloud.google.com Ø¨Ø±ÙˆÛŒØ¯
2. Ù¾Ø±ÙˆÚ˜Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ø³Ø§Ø²ÛŒØ¯ ÛŒØ§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯
3. API Speech-to-Text Ø±Ø§ ÙØ¹Ø§Ù„ Ú©Ù†ÛŒØ¯
4. Service Account Ø¨Ø³Ø§Ø²ÛŒØ¯ Ùˆ JSON key Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯
5. Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set GOOGLE_APPLICATION_CREDENTIALS=path/to/your/key.json

ÛŒØ§ Ø§Ø² Whisper Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ø¢ÙÙ„Ø§ÛŒÙ† Ùˆ Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
            else:
                return f"Google Speech Error: {error_msg}"
    
    def transcribe_with_vosk(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Vosk"""
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Vosk
            if not ModelDownloader.is_model_downloaded(self.model_name):
                # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø¨Ø§ progress bar
                def progress_callback(message):
                    self.download_status.emit(message)
                
                def progress_bar_callback(percent):
                    self.download_progress.emit(percent)
                
                success, result = ModelDownloader.download_model(
                    self.model_name, 
                    progress_callback=progress_callback,
                    progress_bar_callback=progress_bar_callback
                )
                if not success:
                    return f"Vosk Error: {result}"
            
            # Ø¯Ø±ÛŒØ§ÙØª Ù…Ø³ÛŒØ± Ù…Ø¯Ù„
            model_path = self.get_vosk_model_path()
            if not model_path:
                return "Vosk Error: Ù…Ø¯Ù„ Vosk ÛŒØ§ÙØª Ù†Ø´Ø¯."
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
            model = vosk.Model(model_path)
            
            # ØªÙ†Ø¸ÛŒÙ… Ø²Ø¨Ø§Ù† Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
            if self.model_name == "vosk_persian":
                rec = vosk.KaldiRecognizer(model, 16000)
            elif self.model_name in ["vosk_arabic", "vosk_spanish", "vosk_french", "vosk_german", 
                                   "vosk_italian", "vosk_portuguese", "vosk_russian", 
                                   "vosk_chinese", "vosk_japanese", "vosk_korean"]:
                rec = vosk.KaldiRecognizer(model, 16000)
            else:
                rec = vosk.KaldiRecognizer(model, 16000)
            
            # Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            with open(audio_file, "rb") as f:
                data = f.read()
            
            # ØªØ´Ø®ÛŒØµ
            if rec.AcceptWaveform(data):
                result = json.loads(rec.Result())
                text = result.get("text", "")
            else:
                result = json.loads(rec.FinalResult())
                text = result.get("text", "")
            
            return text.strip()
            
        except Exception as e:
            return f"Vosk Error: {str(e)}"
    
    def get_vosk_model_path(self):
        """Ø¯Ø±ÛŒØ§ÙØª Ù…Ø³ÛŒØ± Ù…Ø¯Ù„ Vosk"""
        if self.model_name not in ModelDownloader.VOSK_MODELS:
            return None
        
        model_info = ModelDownloader.VOSK_MODELS[self.model_name]
        models_dir = Path.home() / ".vosk" / "models"
        model_path = models_dir / model_info["name"]
        
        if model_path.exists():
            return str(model_path)
        
        return None
    
    def transcribe_with_azure(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Azure Speech"""
        try:
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Azure (Ù†ÛŒØ§Ø² Ø¨Ù‡ API Key)
            speech_key = os.getenv("AZURE_SPEECH_KEY")
            service_region = os.getenv("AZURE_SPEECH_REGION", "eastus")
            
            if not speech_key:
                return """Azure Speech Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… API Key

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Azure Speech:

1. Ø¨Ù‡ https://portal.azure.com Ø¨Ø±ÙˆÛŒØ¯
2. Cognitive Services > Speech Ø±Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ù†ÛŒØ¯
3. API Key Ùˆ Region Ø±Ø§ Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯
4. Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set AZURE_SPEECH_KEY=your_key_here
   set AZURE_SPEECH_REGION=your_region_here

ÛŒØ§ Ø§Ø² Vosk Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ú©Ø§Ù…Ù„Ø§Ù‹ Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
            
            # ØªÙ†Ø¸ÛŒÙ… speech config
            speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)
            speech_config.speech_recognition_language = "fa-IR"  # ÙØ§Ø±Ø³ÛŒ Ø§ÛŒØ±Ø§Ù†
            
            # ØªÙ†Ø¸ÛŒÙ… audio config
            audio_config = speechsdk.audio.AudioConfig(filename=audio_file)
            
            # Ø§ÛŒØ¬Ø§Ø¯ speech recognizer
            speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)
            
            # ØªØ´Ø®ÛŒØµ
            result = speech_recognizer.recognize_once()
            
            if result.reason == speechsdk.ResultReason.RecognizedSpeech:
                return result.text
            elif result.reason == speechsdk.ResultReason.NoMatch:
                return "Azure Speech: ØªØ´Ø®ÛŒØµ Ø§Ù†Ø¬Ø§Ù… Ù†Ø´Ø¯"
            else:
                return f"Azure Speech Error: {result.reason}"
                
        except Exception as e:
            return f"Azure Speech Error: {str(e)}"
    
    def transcribe_with_assemblyai(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ AssemblyAI"""
        try:
            # ØªÙ†Ø¸ÛŒÙ…Ø§Øª AssemblyAI (Ù†ÛŒØ§Ø² Ø¨Ù‡ API Key)
            api_key = os.getenv("ASSEMBLYAI_API_KEY")
            
            if not api_key:
                return """AssemblyAI Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… API Key

Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² AssemblyAI:

1. Ø¨Ù‡ https://www.assemblyai.com Ø¨Ø±ÙˆÛŒØ¯
2. Ø­Ø³Ø§Ø¨ Ú©Ø§Ø±Ø¨Ø±ÛŒ Ø¨Ø³Ø§Ø²ÛŒØ¯
3. API Key Ø±Ø§ Ú©Ù¾ÛŒ Ú©Ù†ÛŒØ¯
4. Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯:
   set ASSEMBLYAI_API_KEY=your_key_here

ÛŒØ§ Ø§Ø² Vosk Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯ (Ú©Ø§Ù…Ù„Ø§Ù‹ Ø±Ø§ÛŒÚ¯Ø§Ù†)
"""
            
            # ØªÙ†Ø¸ÛŒÙ… API key
            aai.settings.api_key = api_key
            
            # Ø§ÛŒØ¬Ø§Ø¯ transcriber
            transcriber = aai.Transcriber()
            
            # Ø¢Ù¾Ù„ÙˆØ¯ ÙØ§ÛŒÙ„
            transcript = transcriber.transcribe(audio_file)
            
            if transcript.status == aai.TranscriptStatus.completed:
                return transcript.text
            else:
                return f"AssemblyAI Error: {transcript.status}"
                
        except Exception as e:
            return f"AssemblyAI Error: {str(e)}"
    
    def transcribe_with_huggingface(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Hugging Face Transformers"""
        try:
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù†ÙˆØ¹
            if self.model_name == "hf_wav2vec2_persian":
                model_name = "m3hrdadfi/wav2vec2-large-xlsr-53-persian"
                processor = AutoProcessor.from_pretrained(model_name)
                model = AutoModelForCTC.from_pretrained(model_name)
            elif self.model_name == "hf_whisper_persian":
                model_name = "m3hrdadfi/whisper-persian"
                processor = AutoProcessor.from_pretrained(model_name)
                model = AutoModelForCTC.from_pretrained(model_name)
            else:
                # Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¹Ù…ÙˆÙ…ÛŒ Whisper
                model_name = self.model_name.replace("hf_", "").replace("_hf", "")
                if model_name == "whisper_large":
                    model_name = "openai/whisper-large-v2"
                else:
                    model_name = f"openai/whisper-{model_name}"
                
                processor = AutoProcessor.from_pretrained(model_name)
                model = AutoModelForCTC.from_pretrained(model_name)
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            try:
                import librosa
                audio, sr = librosa.load(audio_file, sr=16000)
            except ImportError:
                try:
                    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² soundfile Ø¨Ù‡ Ø¹Ù†ÙˆØ§Ù† Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ†
                    import soundfile as sf
                    audio, sr = sf.read(audio_file)
                    if sr != 16000:
                        # ØªØ¨Ø¯ÛŒÙ„ sample rate Ø¨Ù‡ 16000
                        import numpy as np
                        from scipy import signal
                        audio = signal.resample(audio, int(len(audio) * 16000 / sr))
                        sr = 16000
                except ImportError:
                    return "Hugging Face Error: Neither librosa nor soundfile is available. Install with: pip install librosa soundfile"
            
            # Ù¾Ø±Ø¯Ø§Ø²Ø´
            try:
                inputs = processor(audio, sampling_rate=sr, return_tensors="pt")
                
                # ØªØ´Ø®ÛŒØµ
                with torch.no_grad():
                    logits = model(inputs.input_values).logits
                
                # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ Ù…ØªÙ†
                predicted_ids = torch.argmax(logits, dim=-1)
                text = processor.batch_decode(predicted_ids)[0]
                
                return text.strip()
            except Exception as e:
                return f"Hugging Face Error: Model processing failed - {str(e)}"
            
        except Exception as e:
            return f"Hugging Face Error: {str(e)}"
    
    def transcribe_with_speechrecognition(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ SpeechRecognition"""
        try:
            r = sr.Recognizer()
            
            with sr.AudioFile(audio_file) as source:
                audio = r.record(source)
            
            # Ø§Ù†ØªØ®Ø§Ø¨ Ø³Ø±ÙˆÛŒØ³ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù…Ø¯Ù„
            if self.model_name == "speechrecognition_google":
                text = r.recognize_google(audio, language="fa-IR")
            elif self.model_name == "speechrecognition_sphinx":
                text = r.recognize_sphinx(audio)
            elif self.model_name == "speechrecognition_wit":
                text = r.recognize_wit(audio, key=os.getenv("WIT_AI_KEY"))
            elif self.model_name == "speechrecognition_azure":
                text = r.recognize_azure(audio, key=os.getenv("AZURE_SPEECH_KEY"), location=os.getenv("AZURE_SPEECH_REGION"))
            elif self.model_name == "speechrecognition_bing":
                text = r.recognize_bing(audio, key=os.getenv("BING_KEY"))
            elif self.model_name == "speechrecognition_houndify":
                text = r.recognize_houndify(audio, client_id=os.getenv("HOUNDIFY_CLIENT_ID"), client_key=os.getenv("HOUNDIFY_CLIENT_KEY"))
            elif self.model_name == "speechrecognition_ibm":
                text = r.recognize_ibm(audio, username=os.getenv("IBM_USERNAME"), password=os.getenv("IBM_PASSWORD"))
            else:
                return "SpeechRecognition Error: Unknown service"
            
            return text.strip()
            
        except Exception as e:
            return f"SpeechRecognition Error: {str(e)}"
    
    def transcribe_with_silero(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Silero STT"""
        try:
            import torch
            import torchaudio
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
            if self.model_name == "silero_stt_en":
                model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='en')
            else:  # silero_stt_multilingual
                model, decoder, utils = torch.hub.load(repo_or_dir='snakers4/silero-models', model='silero_stt', language='multilingual')
            
            # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            audio, sample_rate = torchaudio.load(audio_file)
            
            # ØªØ¨Ø¯ÛŒÙ„ Ø¨Ù‡ mono
            if audio.shape[0] > 1:
                audio = torch.mean(audio, dim=0, keepdim=True)
            
            # ØªØ´Ø®ÛŒØµ
            text = decoder(model(audio[0]))
            
            return text.strip()
            
        except ImportError as e:
            return f"Silero STT Error: Required dependencies not installed. Install with: pip install torch torchaudio"
        except Exception as e:
            return f"Silero STT Error: {str(e)}"
    
    def transcribe_with_kaldi(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Kaldi"""
        try:
            # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Kaldi
            if not KALDI_AVAILABLE:
                return "Kaldi Error: kaldi-io not installed. Install with: pip install kaldi-io"
            
            # Ø¨Ø±Ø§ÛŒ KaldiØŒ Ù…Ø§ Ø§Ø² ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ…
            # Ú©Ù‡ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯
            
            if self.model_name == "kaldi_persian":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ ÙØ§Ø±Ø³ÛŒ Kaldi (Ù†ÛŒØ§Ø² Ø¨Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Kaldi)
                return "Kaldi Persian: Ù…Ø¯Ù„ Kaldi ÙØ§Ø±Ø³ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯."
            elif self.model_name == "kaldi_english":
                # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ù…Ø¯Ù„ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Kaldi
                return "Kaldi English: Ù…Ø¯Ù„ Kaldi Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡ Ø¯Ø§Ø±Ø¯. Ù„Ø·ÙØ§Ù‹ Ø§Ø² Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¯ÛŒÚ¯Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯."
            else:
                return "Kaldi Error: Unknown Kaldi model"
            
        except Exception as e:
            return f"Kaldi Error: {str(e)}"
    
    def transcribe_with_iranian(self, audio_file):
        """ØªØ¨Ø¯ÛŒÙ„ ØµÙˆØª Ø¨Ù‡ Ù…ØªÙ† Ø¨Ø§ Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø¨ÙˆÙ…ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ"""
        try:
            # Ø§ÛŒÙ† ÛŒÚ© Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª
            # Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ ÙˆØ§Ù‚Ø¹ÛŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ API key Ù‡Ø§ÛŒ Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø§Ø³Øª
            
            if self.model_name == "iranian_arvan":
                return "Arvan Cloud Speech Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… API Key\nØ¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Arvan Cloud Speech API key Ø®ÙˆØ¯ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯."
            elif self.model_name == "iranian_fanap":
                return "Fanap Speech Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… API Key\nØ¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Fanap Speech API key Ø®ÙˆØ¯ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯."
            elif self.model_name == "iranian_parsijoo":
                return "Parsijoo Speech Error: Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ… API Key\nØ¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Parsijoo Speech API key Ø®ÙˆØ¯ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯."
            else:
                return "Iranian Service Error: Unknown service"
            
        except Exception as e:
            return f"Iranian Service Error: {str(e)}"


class VoiceApp(QWidget):
    def __init__(self):
        super().__init__()
        self.setWindowTitle("Voice Analyzer")
        self.resize(900, 600)  # Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ú©Ø±Ø¯Ù† Ø³Ø§ÛŒØ² Ø§Ù¾
        self.layout = QVBoxLayout(self)

        self.label = QLabel("Select an audio file:")
        self.layout.addWidget(self.label)
        
        self.model_label = QLabel("Selected Model: vosk_persian (default)")
        self.model_label.setStyleSheet("color: blue; font-weight: bold;")
        self.layout.addWidget(self.model_label)

        # Ø¯Ú©Ù…Ù‡ Ù‡Ø§
        self.btn_select = QPushButton("Choose File")
        self.btn_select.setMinimumHeight(40)
        self.btn_select.clicked.connect(self.select_file)
        self.layout.addWidget(self.btn_select)

        self.btn_start_pause = QPushButton("Start / Pause")
        self.btn_start_pause.setMinimumHeight(40)
        self.btn_start_pause.clicked.connect(self.toggle_start_pause)
        self.layout.addWidget(self.btn_start_pause)

        self.btn_stop = QPushButton("Stop")
        self.btn_stop.setMinimumHeight(40)
        self.btn_stop.clicked.connect(self.stop_processing)
        self.layout.addWidget(self.btn_stop)

        self.btn_open_text = QPushButton("Open Text")
        self.btn_open_text.setMinimumHeight(40)
        self.btn_open_text.clicked.connect(self.open_text_file)
        self.layout.addWidget(self.btn_open_text)

        self.btn_manage_dict = QPushButton("Manage Dictionary")
        self.btn_manage_dict.setMinimumHeight(40)
        self.btn_manage_dict.clicked.connect(self.open_dict_manager)
        self.layout.addWidget(self.btn_manage_dict)

        self.btn_google_setup = QPushButton("Google Setup Guide")
        self.btn_google_setup.setMinimumHeight(40)
        self.btn_google_setup.setStyleSheet("background-color: #4285f4; color: white;")
        self.btn_google_setup.clicked.connect(self.show_google_setup_guide)
        self.layout.addWidget(self.btn_google_setup)

        self.btn_download_models = QPushButton("Download Models")
        self.btn_download_models.setMinimumHeight(40)
        self.btn_download_models.setStyleSheet("background-color: #ff6b35; color: white;")
        self.btn_download_models.clicked.connect(self.show_download_models_dialog)
        self.layout.addWidget(self.btn_download_models)

        self.btn_change_model = QPushButton("Change Model")
        self.btn_change_model.setMinimumHeight(40)
        self.btn_change_model.setStyleSheet("background-color: #9c27b0; color: white;")
        self.btn_change_model.clicked.connect(self.change_model)
        self.layout.addWidget(self.btn_change_model)

        self.progress = QProgressBar()
        self.layout.addWidget(self.progress)
        
        # Progress bar Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯
        self.download_progress = QProgressBar()
        self.download_progress.setVisible(False)
        self.download_progress.setStyleSheet("""
            QProgressBar {
                border: 2px solid grey;
                border-radius: 5px;
                text-align: center;
                font-weight: bold;
                font-size: 12px;
            }
            QProgressBar::chunk {
                background-color: #4CAF50;
                border-radius: 3px;
            }
        """)
        self.layout.addWidget(self.download_progress)
        
        # Status label Ø¨Ø±Ø§ÛŒ Ø¯Ø§Ù†Ù„ÙˆØ¯
        self.download_status = QLabel("")
        self.download_status.setVisible(False)
        self.download_status.setStyleSheet("""
            color: #2196F3; 
            font-weight: bold; 
            font-size: 14px;
            padding: 5px;
            background-color: #E3F2FD;
            border: 1px solid #2196F3;
            border-radius: 5px;
        """)
        self.download_status.setAlignment(Qt.AlignCenter)
        self.layout.addWidget(self.download_status)

        self.scroll = QScrollArea()
        self.text_edit = QTextEdit()
        self.text_edit.setReadOnly(True)
        self.text_edit.setFontPointSize(14)  # Ø¨Ø²Ø±Ú¯â€ŒØªØ± Ø´Ø¯Ù† Ù…ØªÙ† Ù†Ù…Ø§ÛŒØ´ÛŒ
        self.scroll.setWidget(self.text_edit)
        self.scroll.setWidgetResizable(True)
        self.layout.addWidget(self.scroll)

        self.audio_path = None
        self.output_file = None
        self.thread = None
        self.dict_manager_window = None
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ùˆ Ù…Ø¯Ù„ Ø¢Ø®Ø±
        config = ConfigManager.load_config()
        self.selected_model = config.get("selected_model", "vosk_persian")
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
        last_audio_path = config.get("last_audio_path", "")
        if last_audio_path and os.path.exists(last_audio_path):
            self.audio_path = last_audio_path
            self.label.setText(f"Last File: {last_audio_path}")
        
        # Ø¨Ø§Ø²Ú¯Ø±Ø¯Ø§Ù†ÛŒ Ù…ÙˆÙ‚Ø¹ÛŒØª Ùˆ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡
        window_geometry = config.get("window_geometry")
        if window_geometry:
            self.setGeometry(
                window_geometry.get("x", 100),
                window_geometry.get("y", 100),
                window_geometry.get("width", 900),
                window_geometry.get("height", 600)
            )
        
        # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„
        self.update_model_display()

    def update_model_display(self):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡"""
        model_info = ModelDownloader.DOWNLOADABLE_MODELS.get(self.selected_model, {})
        if model_info:
            model_name = model_info.get("name", self.selected_model)
            model_type = model_info.get("type", "Unknown")
            model_language = model_info.get("language", "Unknown")
            self.model_label.setText(f"Selected Model: {self.selected_model} ({model_type} - {model_language})")
        else:
            self.model_label.setText(f"Selected Model: {self.selected_model}")

    def select_file(self):
        file_path, _ = QFileDialog.getOpenFileName(
            self, "Select Audio File", "", 
            "Audio Files (*.mp3 *.wav *.m4a *.ogg *.flac)"
        )
        if file_path:
            self.audio_path = file_path
            self.label.setText(f"Selected: {file_path}")
            self.text_edit.clear()
            self.output_file = None
            self.thread = None  # Ø±ÛŒØ³Øª Ú©Ø±Ø¯Ù† thread Ø¨Ø±Ø§ÛŒ ÙØ§ÛŒÙ„ Ø¬Ø¯ÛŒØ¯
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø³ÛŒØ± Ø¢Ø®Ø±ÛŒÙ† ÙØ§ÛŒÙ„ ØµÙˆØªÛŒ
            ConfigManager.update_audio_path(file_path)

    def start_transcription(self):
        if not self.audio_path:
            QMessageBox.warning(self, "Warning", "No audio file selected!")
            return

        # Ù†Ù…Ø§ÛŒØ´ dialog Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø¯Ù„
        dialog = ModelSelectionDialog(self)
        if dialog.exec() == QDialog.Accepted:
            self.selected_model = dialog.get_selected_model()
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
            ConfigManager.update_model(self.selected_model)
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„
            self.update_model_display()
            
            # Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø¶Ø¹ÛŒÙ
            if self.selected_model in ["whisper_tiny", "whisper_base"]:
                reply = QMessageBox.question(
                    self, "Ù‡Ø´Ø¯Ø§Ø±", 
                    f"Ù…Ø¯Ù„ {self.selected_model} Ø¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø¶Ø¹ÛŒÙ Ø§Ø³Øª Ùˆ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù†ØªØ§ÛŒØ¬ Ù†Ø§Ù…Ù†Ø§Ø³Ø¨ÛŒ Ø¨Ø¯Ù‡Ø¯.\nØ¢ÛŒØ§ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒØ¯ØŸ",
                    QMessageBox.Yes | QMessageBox.No,
                    QMessageBox.No
                )
                if reply == QMessageBox.No:
                    return  # Ú©Ø§Ø±Ø¨Ø± Ø§Ù†ØµØ±Ø§Ù Ø¯Ø§Ø¯
            
            # Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Vosk
            if self.selected_model in ["vosk_small", "vosk_large"]:
                reply = QMessageBox.question(
                    self, "Ù‡Ø´Ø¯Ø§Ø± Ø²Ø¨Ø§Ù†", 
                    f"Ù…Ø¯Ù„ {self.selected_model} ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø·Ø±Ø§Ø­ÛŒ Ø´Ø¯Ù‡ Ùˆ ÙØ§Ø±Ø³ÛŒ Ø±Ø§ Ø¨Ù‡ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯!\n\nØ¨Ø±Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø² 'Vosk Persian' Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.\nØ¢ÛŒØ§ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒØ¯ØŸ",
                    QMessageBox.Yes | QMessageBox.No,
                    QMessageBox.No
                )
                if reply == QMessageBox.No:
                    return  # Ú©Ø§Ø±Ø¨Ø± Ø§Ù†ØµØ±Ø§Ù Ø¯Ø§Ø¯
            
            # Ù‡Ø´Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù¾ÙˆÙ„ÛŒ
            paid_models = [
                "google_enhanced", "google_phone_call", "google_medical", "google_video",
                "azure_enhanced", "assemblyai_enhanced"
            ]
            if self.selected_model in paid_models:
                reply = QMessageBox.question(
                    self, "Ù‡Ø´Ø¯Ø§Ø± Ù‡Ø²ÛŒÙ†Ù‡", 
                    f"Ù…Ø¯Ù„ {self.selected_model} Ù¾ÙˆÙ„ÛŒ Ø§Ø³Øª Ùˆ Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.\nØ¢ÛŒØ§ Ù…Ø·Ù…Ø¦Ù† Ù‡Ø³ØªÛŒØ¯ØŸ",
                    QMessageBox.Yes | QMessageBox.No,
                    QMessageBox.No
                )
                if reply == QMessageBox.No:
                    return  # Ú©Ø§Ø±Ø¨Ø± Ø§Ù†ØµØ±Ø§Ù Ø¯Ø§Ø¯
        else:
            return  # Ú©Ø§Ø±Ø¨Ø± cancel Ú©Ø±Ø¯

        self.text_edit.clear()
        self.output_file = None
        self.thread = TranscribeThread(self.audio_path, self.selected_model)
        self.thread.progress.connect(self.progress.setValue)
        self.thread.finished.connect(self.display_result)
        self.thread.download_progress.connect(self.update_download_progress)
        self.thread.download_status.connect(self.update_download_status)
        self.thread.start()

    def update_download_progress(self, percent):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ progress bar Ø¯Ø§Ù†Ù„ÙˆØ¯"""
        self.download_progress.setValue(percent)
        self.download_progress.setFormat(f"Ø¯Ø§Ù†Ù„ÙˆØ¯: {percent}%")
        
        if percent == 100:
            # Ú©Ù…ÛŒ ØµØ¨Ø± Ú©Ù†ÛŒØ¯ ØªØ§ Ú©Ø§Ø±Ø¨Ø± progress Ú©Ø§Ù…Ù„ Ø±Ø§ Ø¨Ø¨ÛŒÙ†Ø¯
            import time
            time.sleep(1)
            self.download_progress.setVisible(False)
            self.download_status.setVisible(False)
    
    def update_download_status(self, message):
        """Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ status Ø¯Ø§Ù†Ù„ÙˆØ¯"""
        self.download_status.setText(f"ğŸ“¥ {message}")
        self.download_status.setVisible(True)
        self.download_progress.setVisible(True)
        self.download_progress.setValue(0)  # Ø´Ø±ÙˆØ¹ Ø§Ø² ØµÙØ±

    def display_result(self, text):
        self.text_edit.setPlainText(text)
        save_path, _ = QFileDialog.getSaveFileName(
            self, "Save Text", "", "Text Files (*.txt)"
        )
        if save_path:
            with open(save_path, "w", encoding="utf-8") as f:
                f.write(text)
            self.output_file = save_path

    def open_text_file(self):
        if self.output_file and os.path.exists(self.output_file):
            webbrowser.open(self.output_file)
        else:
            QMessageBox.warning(self, "Warning", "No saved text file to open!")

    def toggle_start_pause(self):
        if not self.audio_path:
            QMessageBox.warning(self, "Warning", "No audio file selected!")
            return

        if not self.thread:
            self.start_transcription()
        else:
            QMessageBox.information(self, "Info", "Pause / Resume toggle clicked (future implementation).")

    def stop_processing(self):
        if self.thread and self.thread.isRunning():
            QMessageBox.information(self, "Info", "Stop clicked (future implementation).")

    def open_dict_manager(self):
        if not self.dict_manager_window:
            self.dict_manager_window = DictManager()
        self.dict_manager_window.show()
        self.dict_manager_window.raise_()
        self.dict_manager_window.activateWindow()

    def change_model(self):
        """ØªØºÛŒÛŒØ± Ù…Ø¯Ù„ Ø¨Ø¯ÙˆÙ† Ø´Ø±ÙˆØ¹ transcription"""
        dialog = ModelSelectionDialog(self)
        if dialog.exec() == QDialog.Accepted:
            self.selected_model = dialog.get_selected_model()
            
            # Ø°Ø®ÛŒØ±Ù‡ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡
            ConfigManager.update_model(self.selected_model)
            
            # Ø¨Ù‡â€ŒØ±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù…Ø¯Ù„
            self.update_model_display()
            
            QMessageBox.information(self, "Model Changed", f"Model changed to: {self.selected_model}")

    def closeEvent(self, event):
        """Ø°Ø®ÛŒØ±Ù‡ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù‡Ù†Ú¯Ø§Ù… Ø¨Ø³ØªÙ† Ø¨Ø±Ù†Ø§Ù…Ù‡"""
        # Ø°Ø®ÛŒØ±Ù‡ Ù…ÙˆÙ‚Ø¹ÛŒØª Ùˆ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ù¾Ù†Ø¬Ø±Ù‡
        geometry = {
            "x": self.x(),
            "y": self.y(),
            "width": self.width(),
            "height": self.height()
        }
        ConfigManager.update_window_geometry(geometry)
        event.accept()

    def show_google_setup_guide(self):
        from PySide6.QtWidgets import QDialog, QVBoxLayout, QTextEdit, QPushButton, QHBoxLayout
        from PySide6.QtCore import Qt
        from PySide6.QtGui import QTextCursor
        
        dialog = QDialog(self)
        dialog.setWindowTitle("Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Google Speech")
        dialog.setModal(True)
        dialog.resize(600, 500)
        
        layout = QVBoxLayout(dialog)
        
        # Ù…ØªÙ† Ø±Ø§Ù‡Ù†Ù…Ø§
        text_edit = QTextEdit()
        text_edit.setReadOnly(True)
        text_edit.setHtml("""
        <h2>Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Google Speech-to-Text</h2>
        
        <h3>ğŸ”§ Ù…Ø±Ø§Ø­Ù„ ØªÙ†Ø¸ÛŒÙ…:</h3>
        
        <h4>1ï¸âƒ£ Ø§ÛŒØ¬Ø§Ø¯ Ù¾Ø±ÙˆÚ˜Ù‡ Google Cloud:</h4>
        <p>â€¢ Ø¨Ù‡ <a href="https://console.cloud.google.com">Google Cloud Console</a> Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ Ø±ÙˆÛŒ "New Project" Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ù†Ø§Ù… Ù¾Ø±ÙˆÚ˜Ù‡ ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯ Ùˆ "Create" Ø¨Ø²Ù†ÛŒØ¯</p>
        
        <h4>2ï¸âƒ£ ÙØ¹Ø§Ù„â€ŒØ³Ø§Ø²ÛŒ API:</h4>
        <p>â€¢ Ø¯Ø± Ù…Ù†ÙˆØŒ "APIs & Services" > "Library" Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ "Speech-to-Text API" Ø±Ø§ Ø¬Ø³ØªØ¬Ùˆ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ø±ÙˆÛŒ Ø¢Ù† Ú©Ù„ÛŒÚ© Ú©Ø±Ø¯Ù‡ Ùˆ "Enable" Ø¨Ø²Ù†ÛŒØ¯</p>
        
        <h4>3ï¸âƒ£ Ø§ÛŒØ¬Ø§Ø¯ Service Account:</h4>
        <p>â€¢ "APIs & Services" > "Credentials" Ø¨Ø±ÙˆÛŒØ¯</p>
        <p>â€¢ "Create Credentials" > "Service Account" Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Ù†Ø§Ù… Ùˆ ØªÙˆØ¶ÛŒØ­Ø§Øª ÙˆØ§Ø±Ø¯ Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ Role: "Cloud Speech Client" Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯</p>
        
        <h4>4ï¸âƒ£ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù„ÛŒØ¯ JSON:</h4>
        <p>â€¢ Ø±ÙˆÛŒ Service Account Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯</p>
        <p>â€¢ ØªØ¨ "Keys" > "Add Key" > "Create new key"</p>
        <p>â€¢ JSON Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ø±Ø¯Ù‡ Ùˆ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯</p>
        
        <h4>5ï¸âƒ£ ØªÙ†Ø¸ÛŒÙ… Ù…ØªØºÛŒØ± Ù…Ø­ÛŒØ·ÛŒ:</h4>
        <p>â€¢ ÙØ§ÛŒÙ„ JSON Ø±Ø§ Ø¯Ø± Ù¾ÙˆØ´Ù‡ Ø§Ù…Ù† Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯</p>
        <p>â€¢ Ø¯Ø± Command Prompt Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯:</p>
        <p><code>set GOOGLE_APPLICATION_CREDENTIALS=C:\\path\\to\\your\\key.json</code></p>
        
        <h3>ğŸ’¡ Ù†Ú©Ø§Øª Ù…Ù‡Ù…:</h3>
        <p>â€¢ Google Speech 60 Ø¯Ù‚ÛŒÙ‚Ù‡ Ø¯Ø± Ù…Ø§Ù‡ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø§Ø³Øª</p>
        <p>â€¢ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¨ÛŒØ´ØªØ±ØŒ Ù‡Ø²ÛŒÙ†Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯</p>
        <p>â€¢ Whisper Ú©Ø§Ù…Ù„Ø§Ù‹ Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ø¢ÙÙ„Ø§ÛŒÙ† Ø§Ø³Øª</p>
        
        <h3>ğŸ”— Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÛŒØ¯:</h3>
        <p>â€¢ <a href="https://console.cloud.google.com">Google Cloud Console</a></p>
        <p>â€¢ <a href="https://cloud.google.com/speech-to-text/docs">Ù…Ø³ØªÙ†Ø¯Ø§Øª Speech-to-Text</a></p>
        <p>â€¢ <a href="https://cloud.google.com/docs/authentication/external/set-up-adc">Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Authentication</a></p>
        """)
        
        layout.addWidget(text_edit)
        
        # Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
        button_layout = QHBoxLayout()
        
        open_console_btn = QPushButton("Ø¨Ø§Ø² Ú©Ø±Ø¯Ù† Google Cloud Console")
        open_console_btn.setStyleSheet("background-color: #4285f4; color: white; padding: 8px;")
        open_console_btn.clicked.connect(lambda: webbrowser.open("https://console.cloud.google.com"))
        
        open_docs_btn = QPushButton("Ù…Ø³ØªÙ†Ø¯Ø§Øª Speech-to-Text")
        open_docs_btn.setStyleSheet("background-color: #34a853; color: white; padding: 8px;")
        open_docs_btn.clicked.connect(lambda: webbrowser.open("https://cloud.google.com/speech-to-text/docs"))
        
        close_btn = QPushButton("Ø¨Ø³ØªÙ†")
        close_btn.clicked.connect(dialog.accept)
        
        button_layout.addWidget(open_console_btn)
        button_layout.addWidget(open_docs_btn)
        button_layout.addWidget(close_btn)
        
        layout.addLayout(button_layout)
        
        dialog.exec()
    
    def show_download_models_dialog(self):
        """Ù†Ù…Ø§ÛŒØ´ dialog Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§"""
        from PySide6.QtWidgets import QDialog, QVBoxLayout, QListWidget, QPushButton, QHBoxLayout, QLabel, QTabWidget, QWidget
        from PySide6.QtCore import Qt
        
        dialog = QDialog(self)
        dialog.setWindowTitle("Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Speech-to-Text")
        dialog.setModal(True)
        dialog.resize(700, 500)
        
        layout = QVBoxLayout(dialog)
        
        # ØªÙˆØ¶ÛŒØ­Ø§Øª
        info_label = QLabel("Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Ù‚Ø§Ø¨Ù„ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¨Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ú¯ÙØªØ§Ø± (Ú©Ø§Ù…Ù„Ø§Ù‹ Ø±Ø§ÛŒÚ¯Ø§Ù† Ùˆ Ø¢ÙÙ„Ø§ÛŒÙ†):")
        layout.addWidget(info_label)
        
        # Tab widget Ø¨Ø±Ø§ÛŒ Ø¯Ø³ØªÙ‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø¯Ù„â€ŒÙ‡Ø§
        tab_widget = QTabWidget()
        
        # Tab Vosk
        vosk_tab = QWidget()
        vosk_layout = QVBoxLayout(vosk_tab)
        vosk_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "Vosk":
                status = "âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡" if ModelDownloader.is_model_downloaded(model_id) else "âŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                vosk_list.addItem(item_text)
        
        vosk_layout.addWidget(vosk_list)
        tab_widget.addTab(vosk_tab, "Vosk Models")
        
        # Tab Whisper
        whisper_tab = QWidget()
        whisper_layout = QVBoxLayout(whisper_tab)
        whisper_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "Whisper":
                status = "âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡" if ModelDownloader.is_model_downloaded(model_id) else "âŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                whisper_list.addItem(item_text)
        
        whisper_layout.addWidget(whisper_list)
        tab_widget.addTab(whisper_tab, "Whisper Models")
        
        # Tab Hugging Face
        hf_tab = QWidget()
        hf_layout = QVBoxLayout(hf_tab)
        hf_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "HuggingFace":
                status = "âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡" if ModelDownloader.is_model_downloaded(model_id) else "âŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                hf_list.addItem(item_text)
        
        hf_layout.addWidget(hf_list)
        tab_widget.addTab(hf_tab, "Hugging Face Models")
        
        # Tab SpeechRecognition
        sr_tab = QWidget()
        sr_layout = QVBoxLayout(sr_tab)
        sr_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "SpeechRecognition":
                status = "âœ… Ø¢Ù…Ø§Ø¯Ù‡" if True else "âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                sr_list.addItem(item_text)
        
        sr_layout.addWidget(sr_list)
        tab_widget.addTab(sr_tab, "SpeechRecognition")
        
        # Tab Silero
        silero_tab = QWidget()
        silero_layout = QVBoxLayout(silero_tab)
        silero_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "Silero":
                status = "âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡" if ModelDownloader.is_model_downloaded(model_id) else "âŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                silero_list.addItem(item_text)
        
        silero_layout.addWidget(silero_list)
        tab_widget.addTab(silero_tab, "Silero STT")
        
        # Tab Kaldi
        kaldi_tab = QWidget()
        kaldi_layout = QVBoxLayout(kaldi_tab)
        kaldi_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "Kaldi":
                status = "âœ… Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡" if ModelDownloader.is_model_downloaded(model_id) else "âŒ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø´Ø¯Ù‡"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                kaldi_list.addItem(item_text)
        
        kaldi_layout.addWidget(kaldi_list)
        tab_widget.addTab(kaldi_tab, "Kaldi Models")
        
        # Tab Iranian Services
        iranian_tab = QWidget()
        iranian_layout = QVBoxLayout(iranian_tab)
        iranian_list = QListWidget()
        
        for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
            if model_info["type"] == "Iranian":
                status = "âœ… Ø¢Ù…Ø§Ø¯Ù‡" if True else "âŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªÙ†Ø¸ÛŒÙ…"
                item_text = f"{model_info['name']} ({model_info['size']}) - {model_info['warning']} - {status}"
                iranian_list.addItem(item_text)
        
        iranian_layout.addWidget(iranian_list)
        tab_widget.addTab(iranian_tab, "Ø³Ø±ÙˆÛŒØ³â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ")
        
        layout.addWidget(tab_widget)
        
        # Ø°Ø®ÛŒØ±Ù‡ reference Ù‡Ø§
        dialog.vosk_list = vosk_list
        dialog.whisper_list = whisper_list
        dialog.hf_list = hf_list
        dialog.sr_list = sr_list
        dialog.silero_list = silero_list
        dialog.kaldi_list = kaldi_list
        dialog.iranian_list = iranian_list
        dialog.tab_widget = tab_widget
        
        # Ø¯Ú©Ù…Ù‡â€ŒÙ‡Ø§
        button_layout = QHBoxLayout()
        
        download_btn = QPushButton("Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡")
        download_btn.setStyleSheet("background-color: #4CAF50; color: white; padding: 8px;")
        download_btn.clicked.connect(lambda: self.download_selected_model(dialog))
        
        download_all_btn = QPushButton("Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk")
        download_all_btn.setStyleSheet("background-color: #2196F3; color: white; padding: 8px;")
        download_all_btn.clicked.connect(lambda: self.download_all_vosk_models(dialog))
        
        download_whisper_btn = QPushButton("Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper")
        download_whisper_btn.setStyleSheet("background-color: #FF9800; color: white; padding: 8px;")
        download_whisper_btn.clicked.connect(lambda: self.download_all_whisper_models(dialog))
        
        close_btn = QPushButton("Ø¨Ø³ØªÙ†")
        close_btn.clicked.connect(dialog.accept)
        
        button_layout.addWidget(download_btn)
        button_layout.addWidget(download_all_btn)
        button_layout.addWidget(download_whisper_btn)
        button_layout.addWidget(close_btn)
        
        layout.addLayout(button_layout)
        
        dialog.exec()
    
    def download_selected_model(self, dialog):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡"""
        # ØªØ´Ø®ÛŒØµ tab ÙØ¹Ø§Ù„
        current_tab = dialog.tab_widget.currentIndex()
        
        # ØªØ¹ÛŒÛŒÙ† model_list Ùˆ model_type Ø¨Ø± Ø§Ø³Ø§Ø³ tab
        if current_tab == 0:  # Vosk tab
            model_list = dialog.vosk_list
            model_type = "Vosk"
        elif current_tab == 1:  # Whisper tab
            model_list = dialog.whisper_list
            model_type = "Whisper"
        elif current_tab == 2:  # Hugging Face tab
            model_list = dialog.hf_list
            model_type = "HuggingFace"
        elif current_tab == 3:  # SpeechRecognition tab
            model_list = dialog.sr_list
            model_type = "SpeechRecognition"
        elif current_tab == 4:  # Silero tab
            model_list = dialog.silero_list
            model_type = "Silero"
        elif current_tab == 5:  # Kaldi tab
            model_list = dialog.kaldi_list
            model_type = "Kaldi"
        elif current_tab == 6:  # Iranian tab
            model_list = dialog.iranian_list
            model_type = "Iranian"
        else:
            QMessageBox.warning(dialog, "Ù‡Ø´Ø¯Ø§Ø±", "Ù„Ø·ÙØ§Ù‹ ÛŒÚ© tab Ù…Ø¹ØªØ¨Ø± Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")
            return
        
        current_row = model_list.currentRow()
        if current_row == -1:
            QMessageBox.warning(dialog, "Ù‡Ø´Ø¯Ø§Ø±", "Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù…Ø¯Ù„ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")
            return
        
        # Ø¯Ø±ÛŒØ§ÙØª model_id
        model_ids = [key for key, value in ModelDownloader.DOWNLOADABLE_MODELS.items() 
                    if value["type"] == model_type]
        model_id = model_ids[current_row]
        
        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª Ù…Ø¯Ù„
        if model_type in ["SpeechRecognition", "Iranian"]:
            QMessageBox.information(dialog, "Ø§Ø·Ù„Ø§Ø¹Ø§Øª", f"Ù…Ø¯Ù„ {model_id} Ø¢Ù†Ù„Ø§ÛŒÙ† Ø§Ø³Øª Ùˆ Ù†ÛŒØ§Ø²ÛŒ Ø¨Ù‡ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
            return
        
        if ModelDownloader.is_model_downloaded(model_id):
            QMessageBox.information(dialog, "Ø§Ø·Ù„Ø§Ø¹Ø§Øª", "Ø§ÛŒÙ† Ù…Ø¯Ù„ Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù‡ Ø§Ø³Øª.")
            return
        
        # Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù…Ø¯Ù„
        QMessageBox.information(dialog, "Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯", f"Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_id} Ø´Ø±ÙˆØ¹ Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ØµØ¨Ø± Ú©Ù†ÛŒØ¯...")
        
        success, result = ModelDownloader.download_model(model_id)
        
        if success:
            QMessageBox.information(dialog, "Ù…ÙˆÙÙ‚", f"Ù…Ø¯Ù„ {model_id} Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯!")
            dialog.accept()
        else:
            QMessageBox.critical(dialog, "Ø®Ø·Ø§", f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯: {result}")
    
    def download_all_vosk_models(self, dialog):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk"""
        reply = QMessageBox.question(
            dialog, "ØªØ£ÛŒÛŒØ¯", 
            "Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ØŸ\nØ§ÛŒÙ† Ú©Ø§Ø± Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ø¨Ø±Ø¯.",
            QMessageBox.Yes | QMessageBox.No,
            QMessageBox.No
        )
        
        if reply == QMessageBox.Yes:
            QMessageBox.information(dialog, "Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯", "Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk Ø´Ø±ÙˆØ¹ Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ØµØ¨Ø± Ú©Ù†ÛŒØ¯...")
            
            for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
                if model_info["type"] == "Vosk" and not ModelDownloader.is_model_downloaded(model_id):
                    success, result = ModelDownloader.download_model(model_id)
                    if not success:
                        QMessageBox.critical(dialog, "Ø®Ø·Ø§", f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_id}: {result}")
                        return
            
            QMessageBox.information(dialog, "Ù…ÙˆÙÙ‚", "Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Vosk Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯!")
            dialog.accept()
    
    def download_all_whisper_models(self, dialog):
        """Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper"""
        reply = QMessageBox.question(
            dialog, "ØªØ£ÛŒÛŒØ¯", 
            "Ø¢ÛŒØ§ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper Ø±Ø§ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ú©Ù†ÛŒØ¯ØŸ\nØ§ÛŒÙ† Ú©Ø§Ø± Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø²Ù…Ø§Ù† Ø²ÛŒØ§Ø¯ÛŒ Ø¨Ø¨Ø±Ø¯.",
            QMessageBox.Yes | QMessageBox.No,
            QMessageBox.No
        )
        
        if reply == QMessageBox.Yes:
            QMessageBox.information(dialog, "Ø´Ø±ÙˆØ¹ Ø¯Ø§Ù†Ù„ÙˆØ¯", "Ø¯Ø§Ù†Ù„ÙˆØ¯ Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper Ø´Ø±ÙˆØ¹ Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ØµØ¨Ø± Ú©Ù†ÛŒØ¯...")
            
            for model_id, model_info in ModelDownloader.DOWNLOADABLE_MODELS.items():
                if model_info["type"] == "Whisper" and not ModelDownloader.is_model_downloaded(model_id):
                    success, result = ModelDownloader.download_model(model_id)
                    if not success:
                        QMessageBox.critical(dialog, "Ø®Ø·Ø§", f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø§Ù†Ù„ÙˆØ¯ {model_id}: {result}")
                        return
            
            QMessageBox.information(dialog, "Ù…ÙˆÙÙ‚", "Ù‡Ù…Ù‡ Ù…Ø¯Ù„â€ŒÙ‡Ø§ÛŒ Whisper Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø´Ø¯Ù†Ø¯!")
            dialog.accept()


if __name__ == "__main__":
    app = QApplication(sys.argv)
    window = VoiceApp()
    window.show()
    sys.exit(app.exec())
